WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:03.984
[MUSIC PLAYING]

00:00:12.017 --> 00:00:14.100
WILSON WHITE: Good afternoon,
everyone, especially

00:00:14.100 --> 00:00:17.590
for those of you who
are here in California.

00:00:17.590 --> 00:00:20.010
My name is Wilson White,
and I'm on the public policy

00:00:20.010 --> 00:00:23.580
and government relations
team here in California.

00:00:23.580 --> 00:00:26.905
We have an exciting talk for
you today as part of our Talks

00:00:26.905 --> 00:00:30.660
at Google series, as well
as a series of conversations

00:00:30.660 --> 00:00:34.530
we're having around AI ethics
and technology ethics more

00:00:34.530 --> 00:00:35.860
generally.

00:00:35.860 --> 00:00:39.600
So today, I'm honored to
have Professor Yuval Noah

00:00:39.600 --> 00:00:42.180
Harari with us.

00:00:42.180 --> 00:00:48.270
Yuval is an Israeli historian
and a professor at the Hebrew

00:00:48.270 --> 00:00:50.310
University of Jerusalem.

00:00:50.310 --> 00:00:54.360
He is a dynamic speaker,
thinker, and now

00:00:54.360 --> 00:00:57.280
an international
bestselling author.

00:00:57.280 --> 00:00:59.310
He's the author of three books.

00:00:59.310 --> 00:01:02.190
We're going to talk about
each of those books today.

00:01:02.190 --> 00:01:07.110
The first book he published in
2014, "Sapien," which explored

00:01:07.110 --> 00:01:10.440
some of our history as humans.

00:01:10.440 --> 00:01:16.015
His second book in 2016 had an
interesting take on our future

00:01:16.015 --> 00:01:16.515
as humans.

00:01:16.515 --> 00:01:18.120
It was "Homo Deus."

00:01:18.120 --> 00:01:19.950
And then recently
published a new book,

00:01:19.950 --> 00:01:23.050
the "21 Lessons for
the 21st Century,"

00:01:23.050 --> 00:01:26.130
which attempts to grapple
with some of the issues,

00:01:26.130 --> 00:01:30.150
the pressing issues that
we are facing today.

00:01:30.150 --> 00:01:33.360
So we'll talk about some of the
themes in each of those books

00:01:33.360 --> 00:01:35.040
as we go through
our conversation.

00:01:35.040 --> 00:01:39.570
But collectively, his writings
explore very big concepts

00:01:39.570 --> 00:01:42.990
like free will and
consciousness and intelligence.

00:01:42.990 --> 00:01:46.270
So we'll have a lot to
explore with Yuval today.

00:01:46.270 --> 00:01:49.400
So with that, please join me
in welcoming Professor Yuval

00:01:49.400 --> 00:01:49.900
to Google.

00:01:49.900 --> 00:01:52.317
[APPLAUSE]

00:01:52.317 --> 00:01:53.358
YUVAL NOAH HARARI: Hello.

00:02:01.760 --> 00:02:04.160
WILSON WHITE: Thank you,
Professor, for joining us.

00:02:04.160 --> 00:02:06.090
Before getting
started, I have to say

00:02:06.090 --> 00:02:09.150
that when the
announcement went out

00:02:09.150 --> 00:02:14.130
across Google about this
talk, I got several emails

00:02:14.130 --> 00:02:18.240
from many Googlers around
the world who told me

00:02:18.240 --> 00:02:22.080
that they had either read
or are currently reading

00:02:22.080 --> 00:02:24.180
one or multiple of your books.

00:02:24.180 --> 00:02:27.960
So if you are contemplating
a fourth book,

00:02:27.960 --> 00:02:31.500
maybe on the
afterlife, no spoilers

00:02:31.500 --> 00:02:33.690
during this conversation.

00:02:33.690 --> 00:02:39.270
I want to start with maybe
some of the themes in both

00:02:39.270 --> 00:02:41.520
your current book,
"21 Lessons," as well

00:02:41.520 --> 00:02:45.970
as "Homo Deus," because I'm
the father of two young kids.

00:02:45.970 --> 00:02:47.670
I have two daughters,
a five-year-old

00:02:47.670 --> 00:02:48.900
and a three-year-old.

00:02:48.900 --> 00:02:54.810
And the future that you paint
in "Homo Deus" is interesting.

00:02:54.810 --> 00:02:57.240
So I'd like to ask
you, what should I

00:02:57.240 --> 00:02:59.730
be teaching my daughters?

00:02:59.730 --> 00:03:01.770
YUVAL NOAH HARARI:
That nobody knows

00:03:01.770 --> 00:03:04.500
how the world would
look like in 2050,

00:03:04.500 --> 00:03:08.340
except that it will be
very different from today.

00:03:08.340 --> 00:03:13.530
So the most important things
to emphasize in education

00:03:13.530 --> 00:03:17.910
are things like
emotional intelligence

00:03:17.910 --> 00:03:20.430
and mental stability,
because the one thing

00:03:20.430 --> 00:03:23.400
that they will need
for sure is the ability

00:03:23.400 --> 00:03:25.950
to reinvent
themselves repeatedly

00:03:25.950 --> 00:03:27.670
throughout their lives.

00:03:27.670 --> 00:03:29.580
It's really first
time in history

00:03:29.580 --> 00:03:34.080
that we don't really know what
particular skills to teach

00:03:34.080 --> 00:03:35.910
young people,
because we just don't

00:03:35.910 --> 00:03:39.780
know in what kind of
world they will be living.

00:03:39.780 --> 00:03:43.170
But we do know they will
have to reinvent themselves.

00:03:43.170 --> 00:03:45.600
And especially if you think
about something like the job

00:03:45.600 --> 00:03:49.710
market, maybe the greatest
problem they will face

00:03:49.710 --> 00:03:51.470
will be psychological.

00:03:51.470 --> 00:03:53.730
Because at least
beyond a certain age,

00:03:53.730 --> 00:03:58.140
it's very, very difficult for
people to reinvent themselves.

00:03:58.140 --> 00:04:01.410
So we kind of need
to build identities.

00:04:01.410 --> 00:04:05.640
I mean, if previously, if
traditionally people built

00:04:05.640 --> 00:04:11.350
identities like stone houses
with very deep foundations,

00:04:11.350 --> 00:04:16.019
now it makes more sense to build
identities like tents that you

00:04:16.019 --> 00:04:18.570
can fold and move elsewhere.

00:04:18.570 --> 00:04:20.696
Because we don't know where
you will have to move,

00:04:20.696 --> 00:04:21.779
but you will have to move.

00:04:21.779 --> 00:04:22.900
WILSON WHITE: You
will have to move.

00:04:22.900 --> 00:04:24.483
So I may have to go
back to school now

00:04:24.483 --> 00:04:27.585
to learn these things so that
I can teach the next generation

00:04:27.585 --> 00:04:29.580
of humans here.

00:04:29.580 --> 00:04:32.220
In "21 Lessons for
the 21st Century,"

00:04:32.220 --> 00:04:38.220
you tackle several themes
that even we at Google,

00:04:38.220 --> 00:04:41.430
as a company who are on the
leading edge of technology

00:04:41.430 --> 00:04:45.090
and how technology is
being deployed in society,

00:04:45.090 --> 00:04:48.480
we wrestle with some
of the same issues.

00:04:48.480 --> 00:04:51.210
Tell me a bit
about your thoughts

00:04:51.210 --> 00:04:54.540
on why democracy is in crisis.

00:04:54.540 --> 00:04:56.105
That's a theme in
the current book,

00:04:56.105 --> 00:04:57.480
and I want to
explore that a bit.

00:04:57.480 --> 00:05:00.930
Why you think liberal
democracy as we knew

00:05:00.930 --> 00:05:04.620
it is currently in crisis.

00:05:04.620 --> 00:05:06.870
YUVAL NOAH HARARI: Well, the
entire liberal democratic

00:05:06.870 --> 00:05:11.550
system is built on philosophical
ideas we've inherited

00:05:11.550 --> 00:05:14.160
from the 18th century,
especially the idea

00:05:14.160 --> 00:05:20.310
of free will, which
underlies the basic models

00:05:20.310 --> 00:05:23.880
of the liberal world view
like the voter knows best,

00:05:23.880 --> 00:05:26.400
the customer is
always right, beauty

00:05:26.400 --> 00:05:29.550
is in the eye of the
beholder, follow your heart,

00:05:29.550 --> 00:05:31.025
do what feels good.

00:05:31.025 --> 00:05:33.450
All these liberal
models, which are

00:05:33.450 --> 00:05:37.030
the foundation of our
political and economic system.

00:05:37.030 --> 00:05:41.790
They assume that the ultimate
authority is the free choices

00:05:41.790 --> 00:05:43.290
of individuals.

00:05:43.290 --> 00:05:45.720
I mean, there are, of course,
all kinds of limitations

00:05:45.720 --> 00:05:47.820
and boundary cases
and so forth, but when

00:05:47.820 --> 00:05:50.310
push comes to
shove, for instance,

00:05:50.310 --> 00:05:53.880
in the economic field,
then corporations

00:05:53.880 --> 00:05:58.500
will tend to retreat behind
this last line of defense

00:05:58.500 --> 00:06:01.050
that this is what
the customers want.

00:06:01.050 --> 00:06:02.850
The customer is always right.

00:06:02.850 --> 00:06:05.730
If the customers want
it, it can't be wrong.

00:06:05.730 --> 00:06:08.640
Who are you to tell the
customers that they are wrong?

00:06:08.640 --> 00:06:10.580
Now of course, there
are many exceptions,

00:06:10.580 --> 00:06:13.470
but this is the basics
of the free market.

00:06:13.470 --> 00:06:15.360
This is the first and
last thing you learn.

00:06:15.360 --> 00:06:17.730
The customer is always right.

00:06:17.730 --> 00:06:21.600
So the ultimate authority
in the economic field

00:06:21.600 --> 00:06:23.760
is the desires of the customers.

00:06:23.760 --> 00:06:28.320
And this is really based on a
philosophical and metaphysical

00:06:28.320 --> 00:06:34.320
view about free will, that the
desires of the customer, they

00:06:34.320 --> 00:06:38.710
emanate, they represent the
free will of human beings,

00:06:38.710 --> 00:06:41.100
which is the highest
authority in the universe.

00:06:41.100 --> 00:06:43.634
And therefore, we
must abide by them.

00:06:43.634 --> 00:06:45.300
And it's the same in
the political field

00:06:45.300 --> 00:06:47.430
with the voter knows best.

00:06:47.430 --> 00:06:53.260
And this was OK for the
last two or three centuries.

00:06:53.260 --> 00:06:58.599
Because even though free will
was always a myth and not

00:06:58.599 --> 00:06:59.515
a scientific reality--

00:06:59.515 --> 00:07:02.050
I mean, science knows
of only two kinds

00:07:02.050 --> 00:07:04.240
of processes in nature.

00:07:04.240 --> 00:07:07.000
It knows about
deterministic processes

00:07:07.000 --> 00:07:09.430
and it knows about
random processes.

00:07:09.430 --> 00:07:14.260
And their combination results
in probabilistic processes.

00:07:14.260 --> 00:07:19.050
But randomness and probability,
they are not freedom.

00:07:19.050 --> 00:07:23.170
They mean that I can't
predict your actions

00:07:23.170 --> 00:07:26.330
with 100% accuracy, because
there is randomness.

00:07:26.330 --> 00:07:28.880
But a random robot is not free.

00:07:28.880 --> 00:07:33.610
If you connect a robot, say,
to uranium, a piece of uranium,

00:07:33.610 --> 00:07:35.860
and the decisions of
the robot is determined

00:07:35.860 --> 00:07:39.610
by random processes of the
disintegration of uranium

00:07:39.610 --> 00:07:42.880
atoms, so you will never
be able to predict exactly

00:07:42.880 --> 00:07:44.420
what this robot will do.

00:07:44.420 --> 00:07:45.680
But this is not freedom.

00:07:45.680 --> 00:07:48.470
This is just randomness.

00:07:48.470 --> 00:07:51.630
Now this was always true from
a scientific perspective.

00:07:51.630 --> 00:07:55.060
Humans, certainly
they have a will.

00:07:55.060 --> 00:07:56.350
They make decisions.

00:07:56.350 --> 00:07:57.850
They make choices.

00:07:57.850 --> 00:08:00.290
But they are not free
to choose the will.

00:08:00.290 --> 00:08:02.480
The choices are not independent.

00:08:02.480 --> 00:08:05.000
They depend on a
million factors,

00:08:05.000 --> 00:08:10.120
genetic and hormonal and social
and cultural and so forth,

00:08:10.120 --> 00:08:12.490
which we don't choose.

00:08:12.490 --> 00:08:17.320
Now up till now in
history, the humans

00:08:17.320 --> 00:08:24.100
were so complicated that
for a practical perspective,

00:08:24.100 --> 00:08:27.500
it still made sense to
believe in free will,

00:08:27.500 --> 00:08:30.340
because nobody could
understand you better

00:08:30.340 --> 00:08:31.960
than you understand yourself.

00:08:31.960 --> 00:08:37.299
You had this inner realm
of desires and thoughts

00:08:37.299 --> 00:08:41.260
and feelings which you
had privileged access

00:08:41.260 --> 00:08:42.570
to this inner realm.

00:08:42.570 --> 00:08:45.471
WILSON WHITE: Yeah, but that
hasn't changed today, right?

00:08:45.471 --> 00:08:45.970
Like, that--

00:08:45.970 --> 00:08:47.386
YUVAL NOAH HARARI:
It has changed.

00:08:47.386 --> 00:08:48.340
There is no longer--

00:08:48.340 --> 00:08:53.620
the privilege access now belongs
to corporations like Google.

00:08:53.620 --> 00:08:58.150
They can have access to
things happening ultimately

00:08:58.150 --> 00:09:01.870
inside my body and brain,
which I don't know about.

00:09:01.870 --> 00:09:04.420
There is somebody out
there-- and not just one.

00:09:04.420 --> 00:09:07.330
All kinds of corporations and
governments that maybe not

00:09:07.330 --> 00:09:10.960
today, maybe in five years,
10 years, 20 years, they

00:09:10.960 --> 00:09:15.910
will have privileged access
to what's happening inside me.

00:09:15.910 --> 00:09:18.020
More privileged than my access.

00:09:18.020 --> 00:09:21.430
They could understand what
is happening in my brain

00:09:21.430 --> 00:09:24.940
better than I understand it,
which means-- they will never

00:09:24.940 --> 00:09:25.840
be perfect.

00:09:25.840 --> 00:09:26.673
WILSON WHITE: Right.

00:09:26.673 --> 00:09:30.550
But you will, as a
free person, like, you

00:09:30.550 --> 00:09:35.380
will have delegated that
access or that ability

00:09:35.380 --> 00:09:38.320
to this corporation or
this machine or this--

00:09:38.320 --> 00:09:41.780
YUVAL NOAH HARARI: No, you don't
have to give them permission.

00:09:41.780 --> 00:09:45.640
I mean, in some countries maybe
you have no choice at all.

00:09:45.640 --> 00:09:49.060
But even in a democracy
like the United States,

00:09:49.060 --> 00:09:53.380
a lot of the information that
enables an external entity

00:09:53.380 --> 00:09:57.224
to hack you, nobody
asks you whether you

00:09:57.224 --> 00:09:58.390
want to give it away or not.

00:09:58.390 --> 00:10:01.780
Now at present, most
of the data that

00:10:01.780 --> 00:10:07.700
is being collected on humans is
still from the skin outwards.

00:10:07.700 --> 00:10:09.760
We haven't seen nothing yet.

00:10:09.760 --> 00:10:14.170
We are still just at the
tip of this revolution,

00:10:14.170 --> 00:10:17.590
because at present, whether it's
Google and Facebook and Amazon

00:10:17.590 --> 00:10:21.090
or whether it's the government
or whatever, they all

00:10:21.090 --> 00:10:23.680
are trying to
understand people mainly

00:10:23.680 --> 00:10:29.110
on the basis of what I search,
what I buy, where I go,

00:10:29.110 --> 00:10:30.160
who I meet.

00:10:30.160 --> 00:10:31.990
It's all external.

00:10:31.990 --> 00:10:35.950
The really big revolution,
which is coming very quickly,

00:10:35.950 --> 00:10:38.290
will be when the AI
revolution and machine

00:10:38.290 --> 00:10:41.020
learning and all that,
the infotech revolution,

00:10:41.020 --> 00:10:44.530
meets and merges with
the biotech revolution

00:10:44.530 --> 00:10:46.900
and goes under the skin.

00:10:46.900 --> 00:10:49.950
Biometric sensors or
even external devices.

00:10:49.950 --> 00:10:53.510
Now we are developing
the ability, for example,

00:10:53.510 --> 00:10:58.990
to know the blood
pressure of individuals

00:10:58.990 --> 00:11:00.220
just by looking at them.

00:11:00.220 --> 00:11:02.860
You don't need to put
a sensor on a person.

00:11:02.860 --> 00:11:04.900
Just by looking at
the face, you can

00:11:04.900 --> 00:11:07.240
tell, what is the blood
pressure of that individual?

00:11:07.240 --> 00:11:11.590
And by analyzing tiny movements
in the eyes, in the mouth,

00:11:11.590 --> 00:11:16.090
you can tell all kinds of
things from the current mood

00:11:16.090 --> 00:11:16.990
of the person--

00:11:16.990 --> 00:11:19.050
are you angry, are you bored--

00:11:19.050 --> 00:11:22.190
to things like
sexual orientation.

00:11:22.190 --> 00:11:25.960
So we are talking about
a world in which humans

00:11:25.960 --> 00:11:28.210
are no longer a black box.

00:11:28.210 --> 00:11:31.820
Nobody really understands what
happens inside, so we say, OK.

00:11:31.820 --> 00:11:32.650
Free will.

00:11:32.650 --> 00:11:34.390
No, the box is open.

00:11:34.390 --> 00:11:38.710
And it's open to others,
certain others more

00:11:38.710 --> 00:11:41.020
than it is open to-- you
don't understand what's

00:11:41.020 --> 00:11:43.960
happening in your brain,
but some corporation

00:11:43.960 --> 00:11:46.722
or government or organization
could understand that.

00:11:46.722 --> 00:11:48.430
WILSON WHITE: And
that's a theme that you

00:11:48.430 --> 00:11:50.354
explore in "Homo Deus" pretty--

00:11:50.354 --> 00:11:52.270
YUVAL NOAH HARARI: They're
both in "Homo Deus"

00:11:52.270 --> 00:11:54.310
and in "21 Lessons."

00:11:54.310 --> 00:11:58.090
This is like, maybe the most
important thing to understand

00:11:58.090 --> 00:12:00.160
is that this is
really happening.

00:12:00.160 --> 00:12:04.370
And at present, almost all
the attention goes to the AI.

00:12:04.370 --> 00:12:07.377
Like, now I've been on a
two-week tour of the US

00:12:07.377 --> 00:12:08.710
for the publication of the book.

00:12:08.710 --> 00:12:11.370
Everybody wants
to speak about AI.

00:12:11.370 --> 00:12:12.490
Like, AI.

00:12:12.490 --> 00:12:15.460
Previous book, "Homo Deus" came
out, nobody cared about AI.

00:12:15.460 --> 00:12:17.360
Two years later,
it's everywhere.

00:12:17.360 --> 00:12:18.325
WILSON WHITE: It's
the new hot thing.

00:12:18.325 --> 00:12:19.325
YUVAL NOAH HARARI: Yeah.

00:12:19.325 --> 00:12:21.760
And I try to
emphasize, it's not AI.

00:12:21.760 --> 00:12:24.910
The really important thing
is actually the other side.

00:12:24.910 --> 00:12:25.690
It's the biotech.

00:12:25.690 --> 00:12:27.400
It's the combination.

00:12:27.400 --> 00:12:30.970
It's only the combination-- it's
only with the help of biology

00:12:30.970 --> 00:12:34.240
that AI becomes
really revolutionary.

00:12:34.240 --> 00:12:36.740
Because just do a
thought experiment.

00:12:36.740 --> 00:12:40.960
Let's say we had the best, the
most developed AI in the world.

00:12:40.960 --> 00:12:44.650
But humans, we're not animals.

00:12:44.650 --> 00:12:46.930
We're not biochemical
algorithms.

00:12:46.930 --> 00:12:50.890
But they were something
like transcendent souls

00:12:50.890 --> 00:12:53.910
that make decisions
through free will.

00:12:53.910 --> 00:12:57.480
In such a world, AI would
not have mattered much,

00:12:57.480 --> 00:13:01.530
because AI in such a world could
never have replaced teachers

00:13:01.530 --> 00:13:03.080
and lawyers and doctors.

00:13:03.080 --> 00:13:06.420
You could not even
build self-driving cars

00:13:06.420 --> 00:13:07.650
in such a world.

00:13:07.650 --> 00:13:10.680
Because to put a self
driving car on the road,

00:13:10.680 --> 00:13:13.110
you need biology,
not just computers.

00:13:13.110 --> 00:13:15.550
You need to understand humans.

00:13:15.550 --> 00:13:18.870
For example, if somebody's
approaching the road,

00:13:18.870 --> 00:13:22.530
the car needs to tell, is
this an eight-year-old,

00:13:22.530 --> 00:13:25.410
an 18-year-old,
or an 80-year-old,

00:13:25.410 --> 00:13:30.060
and needs to understand
the different behaviors

00:13:30.060 --> 00:13:35.130
of a human child, a human
teenager, and a human adult.

00:13:35.130 --> 00:13:36.550
And this is biology.

00:13:36.550 --> 00:13:40.590
And similarly, to have really
effective self-driving taxis,

00:13:40.590 --> 00:13:44.430
you need the car to
understand a lot of things

00:13:44.430 --> 00:13:45.960
about human psychology.

00:13:45.960 --> 00:13:49.560
The psychology of the passengers
coming in, what they want,

00:13:49.560 --> 00:13:50.830
and so forth.

00:13:50.830 --> 00:13:55.790
So if you take the biotech out
of the equation AI by itself

00:13:55.790 --> 00:13:58.260
won't really go very far.

00:13:58.260 --> 00:14:00.130
WILSON WHITE: So I
want to push you there,

00:14:00.130 --> 00:14:06.300
because I think it's easy
to arrive at a dystopian

00:14:06.300 --> 00:14:07.770
view of what that
world would look

00:14:07.770 --> 00:14:15.450
like with the bio and AI and
cognitive abilities of machines

00:14:15.450 --> 00:14:16.650
when they meet.

00:14:16.650 --> 00:14:18.640
Like, how that
can end up, right?

00:14:18.640 --> 00:14:22.560
And we see that in Hollywood,
and that dystopian view

00:14:22.560 --> 00:14:24.330
is well documented.

00:14:24.330 --> 00:14:28.806
But I want to explore
with you, like,

00:14:28.806 --> 00:14:31.750
what are some of the
benefits of that combination?

00:14:31.750 --> 00:14:36.300
And how can that lead to
an alternative world view

00:14:36.300 --> 00:14:38.301
than what's explored more
deeply in "Homo Deus?"

00:14:38.301 --> 00:14:39.716
YUVAL NOAH HARARI:
Well, it should

00:14:39.716 --> 00:14:41.950
be emphasized that there
are enormous benefits.

00:14:41.950 --> 00:14:44.140
Otherwise, there would
be no temptation.

00:14:44.140 --> 00:14:46.600
If it was only bad,
nobody would do it.

00:14:46.600 --> 00:14:47.980
Google won't research it.

00:14:47.980 --> 00:14:49.720
Nobody would invest in it.

00:14:49.720 --> 00:14:53.200
And it should also be emphasized
that technology is never

00:14:53.200 --> 00:14:54.550
deterministic.

00:14:54.550 --> 00:15:00.030
You can build either paradise
or hell with these technologies.

00:15:00.030 --> 00:15:01.180
They are not just--

00:15:01.180 --> 00:15:03.790
they don't have just
one type of usage.

00:15:03.790 --> 00:15:08.080
And as a historian and as
a social critic and maybe

00:15:08.080 --> 00:15:10.330
philosopher, I
tend to focus more

00:15:10.330 --> 00:15:12.820
on the dangerous
scenarios, simply

00:15:12.820 --> 00:15:17.230
because for obvious
reasons, the entrepreneurs

00:15:17.230 --> 00:15:19.870
and the corporations and
the scientists and engineers

00:15:19.870 --> 00:15:22.660
are developing
these technologies.

00:15:22.660 --> 00:15:26.410
They naturally tend to focus
on the positive scenarios,

00:15:26.410 --> 00:15:28.270
on all the good it can do.

00:15:28.270 --> 00:15:30.580
But yes, definitely
technology, it

00:15:30.580 --> 00:15:32.500
can do a tremendous
amount of good

00:15:32.500 --> 00:15:36.370
to humanity, to take the example
of the self-driving cars.

00:15:36.370 --> 00:15:40.910
So at present, about
1.25 million people

00:15:40.910 --> 00:15:43.990
are killed each year
in traffic accidents.

00:15:43.990 --> 00:15:48.040
More than 90% of these accidents
are because of human errors.

00:15:48.040 --> 00:15:52.270
If we can replace humans
with self-driving cars,

00:15:52.270 --> 00:15:54.980
it's not that we'll
have no car accidents.

00:15:54.980 --> 00:15:56.000
That's impossible.

00:15:56.000 --> 00:16:00.950
But we'll probably save a
million lives every year.

00:16:00.950 --> 00:16:02.800
So this is a tremendous thing.

00:16:02.800 --> 00:16:05.530
And similarly, the
combination of being

00:16:05.530 --> 00:16:08.890
able to understand what's
happening inside my body, this

00:16:08.890 --> 00:16:13.480
also implies that you can
provide people with the best

00:16:13.480 --> 00:16:15.550
health care in history.

00:16:15.550 --> 00:16:18.580
You can, for example,
diagnose diseases

00:16:18.580 --> 00:16:21.400
long before the
person understands

00:16:21.400 --> 00:16:23.020
that there is something wrong.

00:16:23.020 --> 00:16:26.980
At present, the human
mind or human awareness

00:16:26.980 --> 00:16:31.270
is still a very critical
junction in health care.

00:16:31.270 --> 00:16:35.200
Like, if something
happens inside my body

00:16:35.200 --> 00:16:38.980
and I don't know about it,
I won't go to the doctor.

00:16:38.980 --> 00:16:41.440
So if something like,
I don't know, cancer

00:16:41.440 --> 00:16:46.010
is now spreading in my liver
and I still don't feel anything,

00:16:46.010 --> 00:16:47.080
I won't go to the doctor.

00:16:47.080 --> 00:16:48.340
I won't know about it.

00:16:48.340 --> 00:16:51.910
Only when I start feeling
pain and nausea and all kinds

00:16:51.910 --> 00:16:53.570
of things I can't explain.

00:16:53.570 --> 00:16:55.660
So after some time,
I go to the doctor.

00:16:55.660 --> 00:16:57.830
He does all kinds of tests.

00:16:57.830 --> 00:17:01.300
And finally, they discover,
oh, something's wrong.

00:17:01.300 --> 00:17:06.190
And very often,
by that time, it's

00:17:06.190 --> 00:17:08.710
very expensive and painful.

00:17:08.710 --> 00:17:10.450
Not necessarily too
late, but expensive

00:17:10.450 --> 00:17:12.650
and painful to take care of it.

00:17:12.650 --> 00:17:16.960
If I could have an AI
doctor monitoring my body

00:17:16.960 --> 00:17:21.349
24 hours a day with biometric
sensors and so forth,

00:17:21.349 --> 00:17:27.280
it could discover this
long before I feel anything

00:17:27.280 --> 00:17:29.200
at this stage when
it's still very

00:17:29.200 --> 00:17:33.500
cheap and easy and
painless to cure it.

00:17:33.500 --> 00:17:34.375
So this is wonderful.

00:17:34.375 --> 00:17:35.916
WILSON WHITE: But
in that world, it's

00:17:35.916 --> 00:17:37.750
an AI doctor, and
not a human doctor.

00:17:37.750 --> 00:17:42.010
And I think one of
the potential outcomes

00:17:42.010 --> 00:17:46.060
that you warn about is AI or
machines or that combination

00:17:46.060 --> 00:17:51.470
of bio and AI replacing
us, replacing us as humans.

00:17:51.470 --> 00:17:57.040
And I'd like to think that
one thing that makes us human

00:17:57.040 --> 00:18:02.430
is having meaning in life or
having a purpose for living.

00:18:02.430 --> 00:18:05.890
That's kind of a unique
thing that humans have.

00:18:05.890 --> 00:18:08.620
And I don't think it's
something that we would readily

00:18:08.620 --> 00:18:09.820
want to give up, right?

00:18:09.820 --> 00:18:12.220
So as this technology
is evolving

00:18:12.220 --> 00:18:14.470
and we're developing
it, it's likely

00:18:14.470 --> 00:18:17.080
something that we'll
bake in this need

00:18:17.080 --> 00:18:20.620
to have meaning and
purpose in life.

00:18:20.620 --> 00:18:26.170
You talk about in "21 Lessons"
this notion that God is dead,

00:18:26.170 --> 00:18:27.340
or is God back?

00:18:27.340 --> 00:18:32.630
And the role that
religion may play

00:18:32.630 --> 00:18:37.210
in how we progress as humans.

00:18:37.210 --> 00:18:39.730
Is there a place for
that notion of God

00:18:39.730 --> 00:18:42.550
or religion to
capture and secure

00:18:42.550 --> 00:18:45.960
this notion of meaning in
life or purpose in life?

00:18:45.960 --> 00:18:48.670
YUVAL NOAH HARARI: Well, it
all depends on the definitions.

00:18:48.670 --> 00:18:51.130
I mean, there are
many kinds of gods,

00:18:51.130 --> 00:18:53.770
and people understand
very different things

00:18:53.770 --> 00:18:56.050
by the word religion.

00:18:56.050 --> 00:18:59.680
If you think about
God, so usually people

00:18:59.680 --> 00:19:03.160
have very two extremely
different gods in mind

00:19:03.160 --> 00:19:05.280
when they say the word God.

00:19:05.280 --> 00:19:08.990
One god is the cosmic mystery.

00:19:08.990 --> 00:19:13.030
We don't understand why there is
something rather than nothing,

00:19:13.030 --> 00:19:14.740
why the Big Bang happened.

00:19:14.740 --> 00:19:16.434
What is human consciousness?

00:19:16.434 --> 00:19:18.850
There are many things we don't
understand about the world.

00:19:18.850 --> 00:19:22.240
And some people choose
to call these mysteries

00:19:22.240 --> 00:19:23.710
by the name of God.

00:19:23.710 --> 00:19:27.250
God is the reason there is
something rather than nothing.

00:19:27.250 --> 00:19:30.430
God is behind human
consciousness.

00:19:30.430 --> 00:19:34.420
But the most characteristic
thing of that god

00:19:34.420 --> 00:19:38.600
is that we know absolutely
nothing about him,

00:19:38.600 --> 00:19:40.710
her, it, they.

00:19:40.710 --> 00:19:43.060
There is nothing concrete.

00:19:43.060 --> 00:19:44.910
It's a mystery.

00:19:44.910 --> 00:19:46.720
And this is kind
of the god we talk

00:19:46.720 --> 00:19:51.370
about when late at night in the
desert we sit around a campfire

00:19:51.370 --> 00:19:53.410
and we think about
the meaning of life.

00:19:53.410 --> 00:19:54.456
That's one kind of god.

00:19:54.456 --> 00:19:56.080
I have no problem at
all with this god.

00:19:56.080 --> 00:19:56.913
I like it very much.

00:19:56.913 --> 00:19:58.910
[LAUGHTER]

00:19:58.910 --> 00:20:03.880
Then there is another god
which is the petty lawgiver.

00:20:03.880 --> 00:20:06.610
The chief characteristic
of this god,

00:20:06.610 --> 00:20:11.290
we know a lot of extremely
concrete things about that god.

00:20:11.290 --> 00:20:15.820
We know what he thinks about
female dress code, what kind

00:20:15.820 --> 00:20:19.600
of dresses he likes
women to wear.

00:20:19.600 --> 00:20:22.450
We know what he thinks
about sexuality.

00:20:22.450 --> 00:20:25.950
We know what he thinks
about food, about politics,

00:20:25.950 --> 00:20:29.260
and we know these
tiny little things.

00:20:29.260 --> 00:20:35.110
And this is a god people talk
about when they stand around,

00:20:35.110 --> 00:20:37.130
burning a heretic.

00:20:37.130 --> 00:20:39.280
We'll burn you because
you did something

00:20:39.280 --> 00:20:41.470
that this god-- we know
everything about this god,

00:20:41.470 --> 00:20:45.730
and he doesn't like it that
you do this, so we burn you.

00:20:45.730 --> 00:20:49.620
And it's like a
magic trick that when

00:20:49.620 --> 00:20:51.670
you come and talk
about God-- so how

00:20:51.670 --> 00:20:53.700
do you know that God
exists, and so forth?

00:20:53.700 --> 00:20:57.130
People would say, well, the Big
Bang and human consciousness,

00:20:57.130 --> 00:21:01.890
and science can't explain this,
and science can't explain that.

00:21:01.890 --> 00:21:03.460
And this is true.

00:21:03.460 --> 00:21:08.470
And then like a magician
swapping one card for another,

00:21:08.470 --> 00:21:09.460
they will, shh!

00:21:09.460 --> 00:21:13.930
Take out the mystery god and
place the petty lawgiver,

00:21:13.930 --> 00:21:16.600
and you end up with
something strange like,

00:21:16.600 --> 00:21:19.240
because we don't
understand the Big Bang,

00:21:19.240 --> 00:21:21.760
women must dress
with long sleeves

00:21:21.760 --> 00:21:24.640
and men shouldn't
have sex together.

00:21:24.640 --> 00:21:25.930
And what's the connection?

00:21:25.930 --> 00:21:28.820
I mean, how did you
get from here to there?

00:21:28.820 --> 00:21:33.130
So I prefer to use
different terms here.

00:21:33.130 --> 00:21:35.380
And it's the same with religion.

00:21:35.380 --> 00:21:39.280
People understand very
different things with this word.

00:21:39.280 --> 00:21:44.950
I tend to separate
religions from spirituality.

00:21:44.950 --> 00:21:47.800
Spirituality is about questions.

00:21:47.800 --> 00:21:49.860
Religion is about answers.

00:21:49.860 --> 00:21:54.160
Spirituality is when you have
some big question about life

00:21:54.160 --> 00:21:56.920
like, what is humanity?

00:21:56.920 --> 00:21:59.200
What is the good?

00:21:59.200 --> 00:22:00.134
Who am I?

00:22:00.134 --> 00:22:01.550
WILSON WHITE: Our
purpose in life.

00:22:01.550 --> 00:22:02.530
Like, why are we here?

00:22:02.530 --> 00:22:04.363
YUVAL NOAH HARARI: What
should I do in life?

00:22:04.363 --> 00:22:06.580
And this is kind of--
and you go on a quest,

00:22:06.580 --> 00:22:10.420
looking deeply into
these questions.

00:22:10.420 --> 00:22:13.060
And you're willing to
go after these questions

00:22:13.060 --> 00:22:14.710
wherever they take you.

00:22:14.710 --> 00:22:16.491
WILSON WHITE: You
could just Google it.

00:22:16.491 --> 00:22:17.490
YUVAL NOAH HARARI: Yeah.

00:22:17.490 --> 00:22:18.323
Maybe in the future.

00:22:18.323 --> 00:22:20.825
But so far, at least
some of these questions,

00:22:20.825 --> 00:22:23.200
I think when you type, like,
what is the meaning of life,

00:22:23.200 --> 00:22:24.460
you get 42.

00:22:24.460 --> 00:22:29.950
Like, it is the number one
result in Google search.

00:22:29.950 --> 00:22:32.620
So you go on a spiritual quest.

00:22:32.620 --> 00:22:34.440
And religion is
the exact opposite.

00:22:34.440 --> 00:22:37.870
Religion is somebody comes and
tells you, this is the answer.

00:22:37.870 --> 00:22:39.160
You must believe it.

00:22:39.160 --> 00:22:40.930
If you don't
believe this answer,

00:22:40.930 --> 00:22:43.300
then you will burn in
hell after you die,

00:22:43.300 --> 00:22:45.940
or we'll burn you here
even before you die.

00:22:45.940 --> 00:22:47.710
[LAUGHTER]

00:22:47.710 --> 00:22:49.540
And it's really opposite things.

00:22:49.540 --> 00:22:54.700
Now I think that at the
present moment in history,

00:22:54.700 --> 00:22:56.395
spirituality is
probably more important

00:22:56.395 --> 00:22:58.960
than in any previous
time in history,

00:22:58.960 --> 00:23:04.150
because we are now forced to
confront spiritual questions,

00:23:04.150 --> 00:23:06.009
whether we like it or not.

00:23:06.009 --> 00:23:08.050
WILSON WHITE: And do you
think that confrontation

00:23:08.050 --> 00:23:13.285
with those questions, that will
inform how we allow technology

00:23:13.285 --> 00:23:14.410
to develop and be deployed?

00:23:14.410 --> 00:23:17.870
YUVAL NOAH HARARI: Exactly
Now throughout history,

00:23:17.870 --> 00:23:20.690
you always had a small
minority of people

00:23:20.690 --> 00:23:23.840
who was very interested
in the big spiritual

00:23:23.840 --> 00:23:26.130
and philosophical
questions of life,

00:23:26.130 --> 00:23:29.390
and most people just ignored
them and went along with their,

00:23:29.390 --> 00:23:32.030
like, you know, fighting
about who owns this land

00:23:32.030 --> 00:23:36.420
and who this goad herd, to
whom it belongs, and so forth.

00:23:36.420 --> 00:23:38.960
Now we live in a very
unique time in history

00:23:38.960 --> 00:23:43.820
when engineers must tackle
spiritual questions.

00:23:43.820 --> 00:23:48.710
If you are building a
self-driving car, by force,

00:23:48.710 --> 00:23:52.580
you have to deal with
questions like free will.

00:23:52.580 --> 00:23:55.910
By force, you have to deal with
the example everybody gives.

00:23:55.910 --> 00:23:57.290
The self-driving car.

00:23:57.290 --> 00:23:59.300
Suddenly two kids jump--

00:23:59.300 --> 00:24:01.730
running after a ball
jump in front of the car.

00:24:01.730 --> 00:24:05.390
The only way to save the two
kids is to swerve to the side

00:24:05.390 --> 00:24:08.540
and fall off a cliff and
kill the owner of the car who

00:24:08.540 --> 00:24:09.950
is asleep in the backseat.

00:24:09.950 --> 00:24:12.190
What should the car do?

00:24:12.190 --> 00:24:14.210
Now philosophers
have been arguing

00:24:14.210 --> 00:24:16.580
about these questions
for thousands of years

00:24:16.580 --> 00:24:22.010
with very little
impact on human life.

00:24:22.010 --> 00:24:26.150
But engineers, they
are very impatient.

00:24:26.150 --> 00:24:29.300
If you want to put the
self-driving car on the road

00:24:29.300 --> 00:24:33.170
tomorrow or next
year, you need to tell

00:24:33.170 --> 00:24:34.380
the algorithm what to do.

00:24:34.380 --> 00:24:37.190
And the amazing thing
about this question

00:24:37.190 --> 00:24:42.230
now is that whatever you decide,
this will actually happen.

00:24:42.230 --> 00:24:45.440
Previously, with philosophical
discussions, like you had,

00:24:45.440 --> 00:24:48.650
I don't know, Kant and
Schopenhauer and Mill

00:24:48.650 --> 00:24:51.980
discussing this issue,
should I kill the two kids

00:24:51.980 --> 00:24:54.120
or should I sacrifice my life?

00:24:54.120 --> 00:24:57.330
And even if they
reach an agreement--

00:24:57.330 --> 00:25:00.500
and very little impact
on actual behavior.

00:25:00.500 --> 00:25:03.440
Because even if you
agree theoretically,

00:25:03.440 --> 00:25:07.550
this is the right thing to
do, at a time of crisis,

00:25:07.550 --> 00:25:09.560
philosophy has little power.

00:25:09.560 --> 00:25:12.140
You react from
your gut, not from

00:25:12.140 --> 00:25:14.000
your philosophical theories.

00:25:14.000 --> 00:25:18.890
But with a self-driving car,
if you program the algorithm

00:25:18.890 --> 00:25:20.630
to kill the driver--

00:25:20.630 --> 00:25:23.140
and not the driver, the
owner of the car, and not

00:25:23.140 --> 00:25:25.480
the two kids, you
have a guarantee,

00:25:25.480 --> 00:25:27.890
a mathematical
guarantee that this is

00:25:27.890 --> 00:25:31.040
exactly what the car will do.

00:25:31.040 --> 00:25:34.350
So you have to think far more
carefully than ever before,

00:25:34.350 --> 00:25:36.750
what is the right answer?

00:25:36.750 --> 00:25:41.240
So in this sense, very old
spiritual and philosophical

00:25:41.240 --> 00:25:45.980
questions are now practical
questions of engineering,

00:25:45.980 --> 00:25:48.810
which you cannot escape
if you want, for example,

00:25:48.810 --> 00:25:50.870
to put a self-driving
car on the road.

00:25:50.870 --> 00:25:53.600
WILSON WHITE: I want to go back
to this concept of religion

00:25:53.600 --> 00:25:56.190
versus spirituality
and the role they play

00:25:56.190 --> 00:25:58.280
in "Sapiens," your first book.

00:25:58.280 --> 00:26:03.170
You talk about this concept
of human fictions or stories

00:26:03.170 --> 00:26:08.480
that we create as humans, I
guess to get us through life

00:26:08.480 --> 00:26:12.710
and to get us through our
interactions with each other.

00:26:12.710 --> 00:26:15.720
Those fictions, those
stories, as you put it,

00:26:15.720 --> 00:26:17.500
they've served us well.

00:26:17.500 --> 00:26:21.320
They've resulted in a lot
of good for humankind,

00:26:21.320 --> 00:26:26.015
but have also been the
source of wars and conflict

00:26:26.015 --> 00:26:28.370
and human suffering.

00:26:28.370 --> 00:26:30.230
How do you square
that with this moment

00:26:30.230 --> 00:26:34.760
we're in where spirituality
is an integral part in how

00:26:34.760 --> 00:26:39.620
we think about integrating
technology in our lives?

00:26:39.620 --> 00:26:40.620
YUVAL NOAH HARARI: Phew.

00:26:40.620 --> 00:26:42.560
That's a big question.

00:26:42.560 --> 00:26:46.040
Well, so far in
history, in order

00:26:46.040 --> 00:26:49.940
to organize humans
on a large scale,

00:26:49.940 --> 00:26:56.270
you always had to have some
story, some fiction which

00:26:56.270 --> 00:27:01.640
humans invented, but which
enough humans believed in order

00:27:01.640 --> 00:27:03.680
to agree on how to behave.

00:27:03.680 --> 00:27:04.870
It's not just religion.

00:27:04.870 --> 00:27:08.160
This is the obvious example.

00:27:08.160 --> 00:27:10.700
And even religious
people would agree

00:27:10.700 --> 00:27:14.630
that all religions except
one are fictional stories.

00:27:14.630 --> 00:27:15.500
[LAUGH]

00:27:15.500 --> 00:27:18.170
Except for, of
course, my religion.

00:27:18.170 --> 00:27:20.780
If you ask a Jew, then
he will tell you, yes.

00:27:20.780 --> 00:27:21.920
Judaism is the truth.

00:27:21.920 --> 00:27:22.820
That's for sure.

00:27:22.820 --> 00:27:24.950
But all these
billions of Christians

00:27:24.950 --> 00:27:28.160
and Muslims and Hindus, they
believe in fictional stories.

00:27:28.160 --> 00:27:30.890
I mean, all this story about
Jesus rising from the dead

00:27:30.890 --> 00:27:32.810
and being the Son of
God, this is fake news.

00:27:32.810 --> 00:27:34.310
WILSON WHITE: Wait,
that's not true?

00:27:34.310 --> 00:27:36.170
YUVAL NOAH HARARI: If you
ask a Jew, like a rabbi.

00:27:36.170 --> 00:27:38.780
Even though rabbis tend to be,
like-- to hedge their bets.

00:27:38.780 --> 00:27:39.280
[LAUGH]

00:27:39.280 --> 00:27:40.664
So maybe not.

00:27:40.664 --> 00:27:42.080
But then you go
to the Christians.

00:27:42.080 --> 00:27:44.030
They will say, no,
no, no, no, no no.

00:27:44.030 --> 00:27:45.380
This is true.

00:27:45.380 --> 00:27:48.140
But the Muslims, they
believe in fake news.

00:27:48.140 --> 00:27:50.180
All this story about
Muhammad meeting

00:27:50.180 --> 00:27:53.000
the archangel Gabriel and
the Quran coming from Heaven,

00:27:53.000 --> 00:27:54.252
this is all fake news.

00:27:54.252 --> 00:27:56.710
And then the Muslims, they'll
tell you this about Hinduism.

00:27:56.710 --> 00:27:59.480
So even in religion,
it's very clear.

00:27:59.480 --> 00:28:02.690
The more interesting
thing is that the same

00:28:02.690 --> 00:28:06.080
is true in something
in the economy.

00:28:06.080 --> 00:28:09.090
Corporation, you can't
have a modern economy

00:28:09.090 --> 00:28:12.380
without corporations like
Google and without money,

00:28:12.380 --> 00:28:13.640
like dollars.

00:28:13.640 --> 00:28:16.370
But corporations
and currencies, they

00:28:16.370 --> 00:28:19.550
are also just
stories we invented.

00:28:19.550 --> 00:28:23.300
Google has no physical
or biological reality.

00:28:23.300 --> 00:28:28.100
It is a story created
by the powerful shamans

00:28:28.100 --> 00:28:29.450
we call lawyers.

00:28:29.450 --> 00:28:31.130
[LAUGHTER]

00:28:31.130 --> 00:28:34.100
Even if you ask a
lawyer, what is Google,

00:28:34.100 --> 00:28:36.290
like, you push them
to, what is it,

00:28:36.290 --> 00:28:40.152
they will tell you
it's a legal fiction.

00:28:40.152 --> 00:28:40.985
It's not this chair.

00:28:40.985 --> 00:28:42.980
It belongs to Google, I think.

00:28:42.980 --> 00:28:45.620
But it's not it.

00:28:45.620 --> 00:28:46.662
It's not the money.

00:28:46.662 --> 00:28:47.370
It's the manager.

00:28:47.370 --> 00:28:48.620
It's not the workers.

00:28:48.620 --> 00:28:51.410
It's a story created by lawyers.

00:28:51.410 --> 00:28:53.770
And for example,
I mean, if somehow

00:28:53.770 --> 00:28:56.940
with some natural
calamity destroys--

00:28:56.940 --> 00:29:00.680
like, there is an earthquake
and the Googleplex collapses,

00:29:00.680 --> 00:29:02.270
Google still exists.

00:29:02.270 --> 00:29:04.740
Even if many of the workers
and managers are killed,

00:29:04.740 --> 00:29:05.761
it just hires new ones.

00:29:05.761 --> 00:29:06.260
[LAUGHTER]

00:29:06.260 --> 00:29:08.540
And it still has
money in the bank.

00:29:08.540 --> 00:29:12.410
And even if there is no money
in the bank, they can get a loan

00:29:12.410 --> 00:29:14.870
and build new buildings
and hire new people,

00:29:14.870 --> 00:29:16.460
and everything is OK.

00:29:16.460 --> 00:29:19.760
But then if you have
the most powerful shaman

00:29:19.760 --> 00:29:23.600
like the Supreme Court of the
United States comes and says,

00:29:23.600 --> 00:29:25.930
I don't like your story.

00:29:25.930 --> 00:29:30.340
I think you need to be broken
into different fictions.

00:29:30.340 --> 00:29:31.480
Then that's the end.

00:29:31.480 --> 00:29:33.610
WILSON WHITE: So-- so you--

00:29:33.610 --> 00:29:35.440
[LAUGHTER]

00:29:35.440 --> 00:29:36.400
That's a lot to unpack.

00:29:36.400 --> 00:29:38.505
[LAUGHTER]

00:29:38.505 --> 00:29:41.500
So the advent that we're
in now with fake news

00:29:41.500 --> 00:29:46.780
and really seriously
questioning what veracity means

00:29:46.780 --> 00:29:52.570
and how veracity impacts these
kind of foundational things

00:29:52.570 --> 00:29:55.840
that you laid out earlier in
your remarks that have allowed

00:29:55.840 --> 00:30:02.120
us to work with each other,
work across borders, et cetera,

00:30:02.120 --> 00:30:06.450
with this, where you are on this
notion of stories and fictions

00:30:06.450 --> 00:30:11.490
that we have, is this advent of
fake news, is that a reality?

00:30:11.490 --> 00:30:15.360
Is that where we should be in
terms of questioning what's

00:30:15.360 --> 00:30:16.970
true and what's not true?

00:30:16.970 --> 00:30:19.860
YUVAL NOAH HARARI: On the one
hand, fake news is old news.

00:30:19.860 --> 00:30:22.610
We've had them
throughout history,

00:30:22.610 --> 00:30:27.390
and sometimes in much worse
form than what we see today.

00:30:27.390 --> 00:30:29.700
WILSON WHITE: But is
there such thing as truth?

00:30:29.700 --> 00:30:31.533
YUVAL NOAH HARARI: Yes,
there is absolutely.

00:30:31.533 --> 00:30:33.210
I mean, there is reality.

00:30:33.210 --> 00:30:34.740
I mean, you have
all these stories

00:30:34.740 --> 00:30:36.497
people tell about reality.

00:30:36.497 --> 00:30:37.330
WILSON WHITE: I see.

00:30:37.330 --> 00:30:40.620
YUVAL NOAH HARARI: But
ultimately, there is reality.

00:30:40.620 --> 00:30:44.370
The best test of reality that I
know is the test of suffering.

00:30:44.370 --> 00:30:47.460
Suffering is the most
real thing in the world.

00:30:47.460 --> 00:30:49.950
If you want to know
whether a story is

00:30:49.950 --> 00:30:53.190
about a real entity
or a fictional entity,

00:30:53.190 --> 00:30:56.750
you should just ask, can
this entity actually suffer?

00:30:56.750 --> 00:30:58.860
Now Google cannot suffer.

00:30:58.860 --> 00:31:03.600
Even if the stock goes down,
even if a judge comes and says,

00:31:03.600 --> 00:31:07.800
this is a monopoly, you have to
break it up, it doesn't suffer.

00:31:07.800 --> 00:31:12.692
Humans can suffer
like the managers,

00:31:12.692 --> 00:31:16.182
the owners of the stocks, the
employees, they can suffer.

00:31:16.182 --> 00:31:17.140
WILSON WHITE: My girls.

00:31:17.140 --> 00:31:18.140
YUVAL NOAH HARARI: Yeah.

00:31:18.140 --> 00:31:19.470
They can certainly suffer.

00:31:19.470 --> 00:31:23.900
But we know, we can very easily
that Google is just a story

00:31:23.900 --> 00:31:27.270
by this simple test
that it cannot suffer.

00:31:27.270 --> 00:31:28.920
And it's the same of nations.

00:31:28.920 --> 00:31:30.270
It's the same of currencies.

00:31:30.270 --> 00:31:32.910
The dollar is just a
fiction we created.

00:31:32.910 --> 00:31:35.055
The dollar doesn't suffer
if it loses its value.

00:31:35.055 --> 00:31:36.930
WILSON WHITE: Let me
push you on that, right?

00:31:36.930 --> 00:31:40.020
So oftentimes, like
just in the US,

00:31:40.020 --> 00:31:44.040
they say kind of the
system we set up in the US

00:31:44.040 --> 00:31:44.790
is an experiment.

00:31:44.790 --> 00:31:48.000
It's often styled as
an experiment democracy

00:31:48.000 --> 00:31:51.660
with checks and
balances, et cetera.

00:31:51.660 --> 00:31:55.349
Under one view of that, you can
say that that's kind of a story

00:31:55.349 --> 00:31:56.890
that we've created
in America, right?

00:31:56.890 --> 00:32:00.370
We've created this kind
of really nice story.

00:32:00.370 --> 00:32:02.790
But if that was
broken apart, like,

00:32:02.790 --> 00:32:06.190
that entity is not suffering.

00:32:06.190 --> 00:32:12.050
But if that experiment is the
thing, the proper functioning

00:32:12.050 --> 00:32:13.640
of those institutions
and the things

00:32:13.640 --> 00:32:15.306
that support that--
so that's the thing.

00:32:15.306 --> 00:32:17.690
YUVAL NOAH HARARI: We know
that it functions properly

00:32:17.690 --> 00:32:20.540
because it alleviates suffering.

00:32:20.540 --> 00:32:24.050
It provides health care,
it provides safety.

00:32:24.050 --> 00:32:26.630
And if it doesn't,
then we would say

00:32:26.630 --> 00:32:28.724
the experiment doesn't work.

00:32:28.724 --> 00:32:29.390
The experiment--

00:32:29.390 --> 00:32:33.020
WILSON WHITE: So would you say
that experiment is a fiction?

00:32:33.020 --> 00:32:35.360
Or is that experiment reality?

00:32:35.360 --> 00:32:36.020
Is it a thing?

00:32:36.020 --> 00:32:37.394
YUVAL NOAH HARARI:
The experiment

00:32:37.394 --> 00:32:39.830
is a story that we share.

00:32:39.830 --> 00:32:43.940
It's things that we humans
have invented and created

00:32:43.940 --> 00:32:50.450
in order to serve certain
needs and desires that we have.

00:32:50.450 --> 00:32:56.600
It is a created story, and
not an objective reality.

00:32:56.600 --> 00:32:59.459
But it is nevertheless one
of the most powerful forces

00:32:59.459 --> 00:33:00.000
in the world.

00:33:00.000 --> 00:33:02.870
When I say that something
is a fiction or a story,

00:33:02.870 --> 00:33:06.931
I don't mean to imply it's bad
or that it's not important.

00:33:06.931 --> 00:33:07.430
No.

00:33:07.430 --> 00:33:09.530
Some of the best
things in the world

00:33:09.530 --> 00:33:12.040
and the most powerful
forces in the world

00:33:12.040 --> 00:33:15.380
are these shared fictions.

00:33:15.380 --> 00:33:19.850
Nations and corporations
and banks and so forth,

00:33:19.850 --> 00:33:23.510
they are all stories
we created, but they

00:33:23.510 --> 00:33:26.990
are the most powerful
forces today in the world,

00:33:26.990 --> 00:33:31.250
far more powerful than any
human being or any animal.

00:33:31.250 --> 00:33:34.710
And they can be a
tremendous force for good.

00:33:34.710 --> 00:33:39.710
The key is to remember that
we created them to serve us,

00:33:39.710 --> 00:33:43.670
and not that we are here
in order to serve them.

00:33:43.670 --> 00:33:46.310
The trouble really
begins when people

00:33:46.310 --> 00:33:53.720
lose sight of the simple reality
that we are real, they are not.

00:33:53.720 --> 00:33:56.210
And a lot of people
throughout history and also

00:33:56.210 --> 00:33:59.930
today, they kind of
take it upside down.

00:33:59.930 --> 00:34:03.020
They think the nation
is more real than me.

00:34:03.020 --> 00:34:06.680
I am here to serve
it, and not it is here

00:34:06.680 --> 00:34:09.169
to serve me and
my fellow humans.

00:34:09.169 --> 00:34:10.460
WILSON WHITE: Very interesting.

00:34:10.460 --> 00:34:12.553
So we're going to open
it up for questions

00:34:12.553 --> 00:34:14.219
from the audience in
a few minutes here,

00:34:14.219 --> 00:34:17.030
but I want to try
to get an easy win.

00:34:17.030 --> 00:34:22.010
So in "21 Lessons," you
tackle really big challenges

00:34:22.010 --> 00:34:25.489
and questions that we're
wrestling with today.

00:34:25.489 --> 00:34:30.920
Of those questions, which do you
think is the easiest to solve?

00:34:30.920 --> 00:34:33.352
And what should we be doing
to go about solving them?

00:34:33.352 --> 00:34:34.310
YUVAL NOAH HARARI: Ooh.

00:34:34.310 --> 00:34:38.274
What is the easiest to solve?

00:34:38.274 --> 00:34:38.774
[EXHALE]

00:34:38.774 --> 00:34:39.270
[LAUGH]

00:34:39.270 --> 00:34:41.645
WILSON WHITE: Trying to get
quick wins on the board here.

00:34:41.645 --> 00:34:43.855
YUVAL NOAH HARARI: Yeah.

00:34:43.855 --> 00:34:46.840
I'll address the
fake news question,

00:34:46.840 --> 00:34:48.909
not because it's the
easiest to solve, but also

00:34:48.909 --> 00:34:51.790
maybe because it's one of
the most relevant to what

00:34:51.790 --> 00:34:53.770
you're doing here in Google.

00:34:53.770 --> 00:34:58.450
And I would say that the current
incarnation of the fake news

00:34:58.450 --> 00:35:03.760
problem has a lot to do
with the model of the news

00:35:03.760 --> 00:35:10.630
and information market, that we
have constructed a model which

00:35:10.630 --> 00:35:16.330
basically says,
exciting news for free

00:35:16.330 --> 00:35:19.210
in exchange for your attention.

00:35:19.210 --> 00:35:23.030
And this is a very
problematic model,

00:35:23.030 --> 00:35:28.020
because it turns human attention
into the most scarce resource,

00:35:28.020 --> 00:35:32.490
and you get more and more
competition for human attention

00:35:32.490 --> 00:35:35.670
with more and more
exciting news that-- again,

00:35:35.670 --> 00:35:37.980
and some of the smartest
people in the world

00:35:37.980 --> 00:35:40.170
have learned how to
excite our brain,

00:35:40.170 --> 00:35:43.560
how to make us click
on the next news story.

00:35:43.560 --> 00:35:46.140
And truth gets
completely pushed aside.

00:35:46.140 --> 00:35:49.060
It's not part of the equation.

00:35:49.060 --> 00:35:51.090
The equation is
excitement, attention.

00:35:51.090 --> 00:35:52.890
Excitement, attention.

00:35:52.890 --> 00:35:55.950
And on the collective
level, I think

00:35:55.950 --> 00:35:58.140
the solution to
this problem would

00:35:58.140 --> 00:36:02.610
be to change the model
of the news market

00:36:02.610 --> 00:36:07.620
to high-quality news that
costs you a lot of money,

00:36:07.620 --> 00:36:10.120
but don't abuse your attention.

00:36:10.120 --> 00:36:14.130
It's very strange that we are
in a situation when people

00:36:14.130 --> 00:36:16.290
are willing to
pay a lot of money

00:36:16.290 --> 00:36:20.430
for high-quality food
and high-quality cars,

00:36:20.430 --> 00:36:23.220
but not for high-quality news.

00:36:23.220 --> 00:36:27.540
And this has a lot to
do with the architecture

00:36:27.540 --> 00:36:29.170
of the information market.

00:36:29.170 --> 00:36:31.770
And I think there are many
things that you here in Google

00:36:31.770 --> 00:36:36.660
can do in order to help society
change the model of the news

00:36:36.660 --> 00:36:37.560
market.

00:36:37.560 --> 00:36:40.350
WILSON WHITE: I'd want to
continue to explore that,

00:36:40.350 --> 00:36:43.170
and whether that would create,
like, an economic divide

00:36:43.170 --> 00:36:45.760
or exacerbate the
current divide,

00:36:45.760 --> 00:36:48.810
but I'm going to open it up
now for audience questions.

00:36:48.810 --> 00:36:53.260
We have a microphone
here on the side.

00:36:53.260 --> 00:36:53.980
Start with you.

00:36:53.980 --> 00:36:54.690
AUDIENCE: Hi.

00:36:54.690 --> 00:36:55.870
Thank you so much for
writing your books.

00:36:55.870 --> 00:36:57.494
They are completely
wonderful, and I've

00:36:57.494 --> 00:36:59.800
had a joy reading them.

00:36:59.800 --> 00:37:02.640
So one of the things that
you kind of explored here

00:37:02.640 --> 00:37:07.230
is we are facing a couple
of global problems.

00:37:07.230 --> 00:37:13.770
And historically, we have never
created global organizations

00:37:13.770 --> 00:37:17.370
which are responsible for
solving global problems who had

00:37:17.370 --> 00:37:19.295
any ability to enforce them.

00:37:19.295 --> 00:37:20.670
And even when
we've created them,

00:37:20.670 --> 00:37:24.720
they have come after
great tragedies.

00:37:24.720 --> 00:37:28.800
So how can we sort of make
that happen and make somebody

00:37:28.800 --> 00:37:30.540
responsible, and
have the ability

00:37:30.540 --> 00:37:35.810
to have those organizations
enforce those solutions?

00:37:35.810 --> 00:37:36.810
YUVAL NOAH HARARI: Yeah.

00:37:36.810 --> 00:37:40.520
I mean, it's not
going to be easy.

00:37:40.520 --> 00:37:42.600
But I think the
most important thing

00:37:42.600 --> 00:37:47.760
is to change the
public conversation

00:37:47.760 --> 00:37:51.000
and focus it on the
global problems.

00:37:51.000 --> 00:37:55.590
If people focus
on local problems,

00:37:55.590 --> 00:38:00.490
they don't see the need for
effective global cooperation.

00:38:00.490 --> 00:38:04.260
So the first step is to
tell people again and again

00:38:04.260 --> 00:38:05.790
and again, look.

00:38:05.790 --> 00:38:09.660
The three biggest problems
that everybody on the planet

00:38:09.660 --> 00:38:13.680
is now facing are nuclear
war, climate change,

00:38:13.680 --> 00:38:15.600
and technological disruption.

00:38:15.600 --> 00:38:19.440
And even if we are able to
prevent nuclear war and climate

00:38:19.440 --> 00:38:23.280
change, still AI and
biotech are going

00:38:23.280 --> 00:38:28.360
to completely disrupt the job
market and even the human body.

00:38:28.360 --> 00:38:32.280
And we need to figure
out how to regulate this

00:38:32.280 --> 00:38:35.700
and how to prevent the
dystopian consequences,

00:38:35.700 --> 00:38:37.980
and make sure that
the more utopian

00:38:37.980 --> 00:38:39.720
consequences materialize.

00:38:39.720 --> 00:38:41.700
And for that, we need
global cooperation.

00:38:41.700 --> 00:38:44.040
So it would be
obvious to everybody,

00:38:44.040 --> 00:38:49.720
you cannot prevent climate
change on a national level,

00:38:49.720 --> 00:38:53.170
and you cannot regulate
AI on a national level.

00:38:53.170 --> 00:38:55.570
Whatever regulation
the US adopts,

00:38:55.570 --> 00:38:59.390
if the Chinese are not adopting
it, it won't do much help.

00:38:59.390 --> 00:39:03.130
So you need cooperation here.

00:39:03.130 --> 00:39:06.230
And then it goes into
practical political issues.

00:39:06.230 --> 00:39:07.960
I mean, you have
elections coming up,

00:39:07.960 --> 00:39:09.700
mid-term elections in the US.

00:39:09.700 --> 00:39:14.110
So if you go to a town meeting
with an inspiring congressman

00:39:14.110 --> 00:39:19.060
or congresswoman, so you just
ask them, if I elect you,

00:39:19.060 --> 00:39:22.120
what will you do about the
danger of climate change,

00:39:22.120 --> 00:39:25.750
about the danger of nuclear
war, and about getting

00:39:25.750 --> 00:39:28.990
global regulations for
AI and for biotech?

00:39:28.990 --> 00:39:30.340
What's your plan?

00:39:30.340 --> 00:39:33.620
And if they say, oh, I
haven't thought about it,

00:39:33.620 --> 00:39:36.820
then maybe don't
vote for that person.

00:39:36.820 --> 00:39:38.200
[LAUGHTER]

00:39:39.580 --> 00:39:41.880
WILSON WHITE: Question.

00:39:41.880 --> 00:39:43.070
AUDIENCE: Hi, Yuval.

00:39:43.070 --> 00:39:44.900
Thanks for coming here today.

00:39:44.900 --> 00:39:47.140
So in one of your
talks, you suggested

00:39:47.140 --> 00:39:50.590
that to avoid getting
our hearts hacked,

00:39:50.590 --> 00:39:54.140
we need to stay ahead by
knowing ourselves better.

00:39:54.140 --> 00:39:56.590
And it seems to me that the
process of knowing yourself

00:39:56.590 --> 00:39:58.390
needs a lot of intelligence.

00:39:58.390 --> 00:40:01.780
And in some ways, it's a
skill that needs to developed.

00:40:01.780 --> 00:40:04.600
I mean, the intellect
that we have as humans

00:40:04.600 --> 00:40:07.940
seems fairly new when
compared to other properties

00:40:07.940 --> 00:40:10.220
like we got evolutionarily.

00:40:10.220 --> 00:40:12.280
So how do you
suggest that we can

00:40:12.280 --> 00:40:15.850
learn to think and use our
intelligence better, and also

00:40:15.850 --> 00:40:17.420
do that at a scale?

00:40:17.420 --> 00:40:20.170
Because if only some
people know themselves

00:40:20.170 --> 00:40:22.300
but millions around you
or billions or on the

00:40:22.300 --> 00:40:25.534
don't, then you
can only go so far.

00:40:25.534 --> 00:40:27.950
YUVAL NOAH HARARI: No, I don't
think that knowing yourself

00:40:27.950 --> 00:40:31.720
is necessarily all
about intelligence.

00:40:31.720 --> 00:40:34.285
Certainly not in the narrow
sense of intelligence.

00:40:34.285 --> 00:40:37.720
If you include emotional
intelligence and so forth,

00:40:37.720 --> 00:40:38.410
then yes.

00:40:38.410 --> 00:40:44.340
But in the more narrow sense
of IQ, I think this is not--

00:40:44.340 --> 00:40:46.930
there are many very
intelligent people

00:40:46.930 --> 00:40:48.730
in the world who
don't know themselves

00:40:48.730 --> 00:40:53.840
at all, which is an extremely
dangerous combination.

00:40:53.840 --> 00:40:59.060
Now some people explore
themselves through therapy.

00:40:59.060 --> 00:41:00.560
Some use meditation.

00:41:00.560 --> 00:41:01.730
Some use art.

00:41:01.730 --> 00:41:02.870
Some use poems.

00:41:02.870 --> 00:41:08.630
They go on a long hike, go for
a month to the Appalachian Trail

00:41:08.630 --> 00:41:10.620
and get to know
themselves on the way.

00:41:10.620 --> 00:41:13.730
There are many ways to do
it, which are not necessarily

00:41:13.730 --> 00:41:15.170
about intellect.

00:41:15.170 --> 00:41:20.210
It's not like reading
articles about brain science.

00:41:20.210 --> 00:41:22.670
That's going to
help in some ways.

00:41:22.670 --> 00:41:24.740
And in this sense, I
think it's a very kind

00:41:24.740 --> 00:41:32.330
of democratizing ability or
force to get to know yourself.

00:41:32.330 --> 00:41:35.670
After all, you-- you're
always with yourself.

00:41:35.670 --> 00:41:38.510
It's not like you need some
special observatory and to get

00:41:38.510 --> 00:41:42.511
some very rare machines
from, I don't know,

00:41:42.511 --> 00:41:43.760
that cost millions of dollars.

00:41:43.760 --> 00:41:45.505
You just need yourself.

00:41:45.505 --> 00:41:46.130
AUDIENCE: Sure.

00:41:46.130 --> 00:41:48.810
But what about the
art of thinking?

00:41:48.810 --> 00:41:50.060
YUVAL NOAH HARARI: What about?

00:41:50.060 --> 00:41:52.580
AUDIENCE: The art of thinking.

00:41:52.580 --> 00:41:54.590
YUVAL NOAH HARARI:
The art of thinking.

00:41:54.590 --> 00:41:56.506
AUDIENCE: I mean, people
are very intelligent,

00:41:56.506 --> 00:41:58.490
but they don't really
use their intelligence

00:41:58.490 --> 00:42:00.073
to understand
themselves [INAUDIBLE]..

00:42:00.073 --> 00:42:01.310
YUVAL NOAH HARARI: Yeah.

00:42:01.310 --> 00:42:04.130
Again, there is no
easy way to do it.

00:42:04.130 --> 00:42:07.070
If it was easy to get
to know yourself better,

00:42:07.070 --> 00:42:09.290
everybody would do
it long ago, and we

00:42:09.290 --> 00:42:13.377
would be living in a very,
very different world.

00:42:13.377 --> 00:42:14.960
WILSON WHITE: We
have folks joining us

00:42:14.960 --> 00:42:17.876
from around the world as
well, so I have a question

00:42:17.876 --> 00:42:20.200
from the question bank.

00:42:20.200 --> 00:42:22.090
Compassion is the
critical underpinning

00:42:22.090 --> 00:42:24.610
of any successful
society, yet I believe

00:42:24.610 --> 00:42:28.410
that technology is reducing
our capacity for empathy.

00:42:28.410 --> 00:42:31.540
It feels that we no longer value
compassion, perhaps even seeing

00:42:31.540 --> 00:42:33.520
compassion as weak.

00:42:33.520 --> 00:42:35.380
What are, in your
view, effective ways

00:42:35.380 --> 00:42:38.800
to motivate members of society
to develop their compassion?

00:42:38.800 --> 00:42:40.300
YUVAL NOAH HARARI:
No, I don't think

00:42:40.300 --> 00:42:44.800
that technology is inherently
undermining compassion.

00:42:44.800 --> 00:42:47.500
It can go both ways.

00:42:47.500 --> 00:42:51.430
Certainly,
communication technology

00:42:51.430 --> 00:42:54.340
can make you aware of
the plight of people

00:42:54.340 --> 00:42:56.320
on the other side of the world.

00:42:56.320 --> 00:42:58.810
And without that,
you may be extremely

00:42:58.810 --> 00:43:02.440
compassionate about your
immediate, like, family members

00:43:02.440 --> 00:43:05.410
and neighbors, and won't
care at all about people

00:43:05.410 --> 00:43:07.040
on the other side of the world.

00:43:07.040 --> 00:43:10.240
So I don't think there is
an inherent contradiction

00:43:10.240 --> 00:43:15.370
or collision between
technology and compassion.

00:43:15.370 --> 00:43:21.520
But it is true that the
way we design technology

00:43:21.520 --> 00:43:26.020
can make us less
compassionate, and even the way

00:43:26.020 --> 00:43:29.110
that we design ourselves.

00:43:29.110 --> 00:43:33.400
For most of history, you had
economic and political systems

00:43:33.400 --> 00:43:35.980
trying to shape people.

00:43:35.980 --> 00:43:38.710
And in the past, they
did it with education

00:43:38.710 --> 00:43:41.110
and with culture.

00:43:41.110 --> 00:43:43.240
And in the present
and future, we

00:43:43.240 --> 00:43:44.800
are likely to do
it more and more

00:43:44.800 --> 00:43:49.480
with biotech and with
brain computer interfaces.

00:43:49.480 --> 00:43:54.160
So our ability to manipulate
ourselves is growing.

00:43:54.160 --> 00:43:56.800
And therefore, it's
extremely important

00:43:56.800 --> 00:44:01.520
to remember to take
compassion into account.

00:44:01.520 --> 00:44:05.500
Otherwise, the danger is
that armies and corporations

00:44:05.500 --> 00:44:07.540
and government in
many cases, they

00:44:07.540 --> 00:44:09.760
want something
like intelligence.

00:44:09.760 --> 00:44:13.270
They want more intelligent
workers and soldiers.

00:44:13.270 --> 00:44:15.220
They want more decisive workers.

00:44:15.220 --> 00:44:17.560
And sort of, don't take
a whole day to decide.

00:44:17.560 --> 00:44:20.960
I want you to decide
this in half an hour.

00:44:20.960 --> 00:44:23.670
And as our ability to
manipulate humans--

00:44:23.670 --> 00:44:25.450
and I mean manipulate--

00:44:25.450 --> 00:44:29.680
re-engineer the body and
the brain as it grows--

00:44:29.680 --> 00:44:35.860
we might engineer more
decisive and intelligent humans

00:44:35.860 --> 00:44:38.230
at the price of compassion.

00:44:38.230 --> 00:44:42.040
Which many corporations
and armies and governments

00:44:42.040 --> 00:44:46.240
find either irrelevant
or even problematic,

00:44:46.240 --> 00:44:48.145
because it causes
people to be hesitant

00:44:48.145 --> 00:44:50.470
and to take more time
about the decisions,

00:44:50.470 --> 00:44:52.640
and so on and so forth.

00:44:52.640 --> 00:44:56.440
So we need to remember
the enormous importance

00:44:56.440 --> 00:44:58.430
of compassion.

00:44:58.430 --> 00:45:00.910
And again, it goes back
also to the question

00:45:00.910 --> 00:45:04.030
about getting to
know yourself, which

00:45:04.030 --> 00:45:07.140
I think is the key to
developing compassion.

00:45:07.140 --> 00:45:10.280
Not just because when
you understand your own,

00:45:10.280 --> 00:45:13.150
that this makes me miserable,
then you understand, oh.

00:45:13.150 --> 00:45:16.090
The same thing may make
other people also miserable.

00:45:16.090 --> 00:45:19.790
It's even much deeper than that.

00:45:19.790 --> 00:45:21.950
When you really get
to know yourself,

00:45:21.950 --> 00:45:26.150
you realize that when
you ignore others

00:45:26.150 --> 00:45:31.420
and when you mistreat others,
very often, it harms you

00:45:31.420 --> 00:45:33.760
even before it harms them.

00:45:33.760 --> 00:45:38.510
It's a very unpleasant
experience to be angry.

00:45:38.510 --> 00:45:42.080
So your anger may harm
other people, or maybe not.

00:45:42.080 --> 00:45:44.990
Maybe you're boiling with
anger about somebody,

00:45:44.990 --> 00:45:48.890
and you don't do anything about
it because she's your boss.

00:45:48.890 --> 00:45:53.400
But you don't harm her,
but your anger harms you.

00:45:53.400 --> 00:45:58.760
So the more you understand
yourself, the greater incentive

00:45:58.760 --> 00:46:03.500
you have to do something about
my anger, about my hatred,

00:46:03.500 --> 00:46:04.790
about my fear.

00:46:04.790 --> 00:46:08.720
And most people discover that
as they develop more compassion

00:46:08.720 --> 00:46:12.800
towards others, they also
experience far more peace

00:46:12.800 --> 00:46:14.167
within themselves.

00:46:14.167 --> 00:46:15.480
WILSON WHITE: Wow.

00:46:15.480 --> 00:46:18.080
Another live question.

00:46:18.080 --> 00:46:18.952
AUDIENCE: Thank you.

00:46:18.952 --> 00:46:20.660
After reading your
books, it occurs to me

00:46:20.660 --> 00:46:23.870
that you've most likely
educated yourself both broadly

00:46:23.870 --> 00:46:26.107
and deeply to be the
foundation for your ideas.

00:46:26.107 --> 00:46:28.190
For those of us that are
interested in cultivating

00:46:28.190 --> 00:46:30.020
our mind similarly,
wondering if you could share

00:46:30.020 --> 00:46:31.603
a little bit about
your reading habits

00:46:31.603 --> 00:46:33.650
and how you choose
what to consume.

00:46:33.650 --> 00:46:35.750
YUVAL NOAH HARARI:
My reading habits.

00:46:35.750 --> 00:46:37.670
I read very eclectically.

00:46:37.670 --> 00:46:45.080
Like, no book is barred
from entering the book list.

00:46:45.080 --> 00:46:49.370
But then I tend to be extremely
impatient about the books I

00:46:49.370 --> 00:46:50.570
actually read.

00:46:50.570 --> 00:46:53.870
I would begin, like, 10
books and drop nine of them

00:46:53.870 --> 00:46:56.630
after 10 pages.

00:46:56.630 --> 00:46:59.060
It's not always
the wisest policy,

00:46:59.060 --> 00:47:05.540
but it's my policy that if a
book didn't really teach me

00:47:05.540 --> 00:47:08.210
something new, had some
interesting insight

00:47:08.210 --> 00:47:11.900
in the first 10 pages,
the chances it will--

00:47:11.900 --> 00:47:14.780
it could be that
on page 100 there

00:47:14.780 --> 00:47:18.500
will be some mind-blowing
idea that I'm now missing.

00:47:18.500 --> 00:47:20.180
But there are so many--

00:47:20.180 --> 00:47:23.060
I keep thinking, there
are so many books,

00:47:23.060 --> 00:47:27.170
wonderful books out there
that I will never read,

00:47:27.170 --> 00:47:33.290
so why waste time on
the less optimal book?

00:47:33.290 --> 00:47:36.170
So I will try, like, a book
on biology and then economics

00:47:36.170 --> 00:47:39.380
and then psychology and
then fiction and whatever,

00:47:39.380 --> 00:47:42.050
and just go through them
quite quickly until I find

00:47:42.050 --> 00:47:43.430
something that really grabs me.

00:47:46.000 --> 00:47:47.975
WILSON WHITE: Another
live question.

00:47:47.975 --> 00:47:49.424
AUDIENCE: Hi, Mr. Harari.

00:47:49.424 --> 00:47:50.340
Thanks for being here.

00:47:50.340 --> 00:47:53.340
Fascinating talk as always.

00:47:53.340 --> 00:47:55.050
I do a little bit of
meditation myself,

00:47:55.050 --> 00:47:57.390
and I've heard that you
do a lot of meditation

00:47:57.390 --> 00:47:58.770
on the order of hours a day.

00:47:58.770 --> 00:48:00.720
Is that right?

00:48:00.720 --> 00:48:03.250
YUVAL NOAH HARARI: I try
to do two hours every day,

00:48:03.250 --> 00:48:08.105
and I try to go every year to a
long retreat of 45 or 60 days.

00:48:08.105 --> 00:48:09.480
AUDIENCE: So I
was wondering, how

00:48:09.480 --> 00:48:12.270
do you feel that has influenced
your life and the ideas

00:48:12.270 --> 00:48:14.200
that you have?

00:48:14.200 --> 00:48:18.270
YUVAL NOAH HARARI: Oh, it's
had a tremendous influence,

00:48:18.270 --> 00:48:21.600
I think both on my
inner peace of mind,

00:48:21.600 --> 00:48:24.480
but also on my work
as a scientist.

00:48:24.480 --> 00:48:27.000
And maybe the two most
important influences

00:48:27.000 --> 00:48:30.420
is that first it
enabled me to have

00:48:30.420 --> 00:48:32.910
more clarity and more focus.

00:48:32.910 --> 00:48:36.240
And certainly when you write
about such big subjects

00:48:36.240 --> 00:48:40.560
like trying to summarize the
whole of history in 400 pages.

00:48:40.560 --> 00:48:43.680
So having a very,
very focused mind

00:48:43.680 --> 00:48:47.145
is very important, because
the great difficulty

00:48:47.145 --> 00:48:50.760
is that everything
kind of distracts you.

00:48:50.760 --> 00:48:53.401
You start writing
about the Roman Empire

00:48:53.401 --> 00:48:54.900
and you say, well,
I have to explain

00:48:54.900 --> 00:48:57.540
this and this and this
and this, and you end up

00:48:57.540 --> 00:48:59.400
with 4,000 pages.

00:48:59.400 --> 00:49:01.950
So we have to be very--
what is really important,

00:49:01.950 --> 00:49:03.810
and what can be left outside?

00:49:03.810 --> 00:49:07.480
And the other thing is that
at least the meditation

00:49:07.480 --> 00:49:10.350
that I practice, which is
with passive meditation,

00:49:10.350 --> 00:49:13.170
it's all about really
knowing the difference

00:49:13.170 --> 00:49:16.290
between the fictions
and stories generated

00:49:16.290 --> 00:49:18.900
by our mind and the reality.

00:49:18.900 --> 00:49:22.290
What is really
happening right now?

00:49:22.290 --> 00:49:26.940
And when I meditate,
the thing that happens

00:49:26.940 --> 00:49:31.440
is that constantly, the mind is
like a factory that constantly

00:49:31.440 --> 00:49:36.580
generates stories about
myself, about other people,

00:49:36.580 --> 00:49:38.280
about the world.

00:49:38.280 --> 00:49:39.860
And they are very attractive.

00:49:39.860 --> 00:49:42.670
Like, I get
identified with them.

00:49:42.670 --> 00:49:45.000
And the meditation
is constantly, don't.

00:49:45.000 --> 00:49:46.350
It's just a story.

00:49:46.350 --> 00:49:47.760
Leave it.

00:49:47.760 --> 00:49:53.230
Just try to stay with what is
really happening right now.

00:49:53.230 --> 00:49:56.680
And this is the central
practice in meditation.

00:49:56.680 --> 00:50:01.540
It's also a guiding principle
when I study history

00:50:01.540 --> 00:50:04.214
or when I study what's
happening in the world.

00:50:04.214 --> 00:50:04.880
AUDIENCE: Great.

00:50:04.880 --> 00:50:05.990
Thank you.

00:50:05.990 --> 00:50:08.600
WILSON WHITE: Let's take
another question from the Dory.

00:50:08.600 --> 00:50:10.950
With inequality rising
across most nations

00:50:10.950 --> 00:50:13.400
in the last few decades,
what is your perspective

00:50:13.400 --> 00:50:17.540
on how we can use technological
growth to solve this problem

00:50:17.540 --> 00:50:19.370
and create a more
equitable world?

00:50:19.370 --> 00:50:23.262
Do we need a different economic
paradigm to achieve this?

00:50:23.262 --> 00:50:24.720
YUVAL NOAH HARARI:
Yes, we probably

00:50:24.720 --> 00:50:27.960
need a different economic
paradigm, because we

00:50:27.960 --> 00:50:30.840
are entering kind
of uncharted waters,

00:50:30.840 --> 00:50:33.450
especially because of
the automation revolution

00:50:33.450 --> 00:50:37.770
and the growing likelihood
that more and more people might

00:50:37.770 --> 00:50:40.650
be completely pushed
out of the job market,

00:50:40.650 --> 00:50:43.890
not just because there
won't be enough jobs,

00:50:43.890 --> 00:50:48.150
but simply because the pace
of change in the job market

00:50:48.150 --> 00:50:49.630
will accelerate.

00:50:49.630 --> 00:50:52.530
So even if there
are enough jobs,

00:50:52.530 --> 00:50:55.680
people don't have the
psychological balance

00:50:55.680 --> 00:50:59.880
and stamina to constantly
retrain, reskill, or reinvent

00:50:59.880 --> 00:51:02.260
themselves.

00:51:02.260 --> 00:51:04.530
And so I think the biggest
problem in the job market

00:51:04.530 --> 00:51:08.070
is really going to be the
psychological problem.

00:51:08.070 --> 00:51:11.370
And then what do you do when
more and more people are

00:51:11.370 --> 00:51:12.780
left out?

00:51:12.780 --> 00:51:15.720
And there are
explorations of new models

00:51:15.720 --> 00:51:19.110
like universal basic
income and so forth, which

00:51:19.110 --> 00:51:20.400
are worth exploring.

00:51:20.400 --> 00:51:22.150
I don't have the answers.

00:51:22.150 --> 00:51:24.810
I will just say that
anybody who thinks

00:51:24.810 --> 00:51:27.210
in terms like
universal basic income

00:51:27.210 --> 00:51:31.560
should take the word universal
very, very seriously,

00:51:31.560 --> 00:51:36.330
and not settle for
national basic income.

00:51:36.330 --> 00:51:39.390
Because the greatest
inequality we

00:51:39.390 --> 00:51:44.010
are facing will probably be
inequality between countries,

00:51:44.010 --> 00:51:46.170
and not within countries.

00:51:46.170 --> 00:51:50.580
Some countries are likely
to become extremely wealthy

00:51:50.580 --> 00:51:53.160
due to the automation
revolution,

00:51:53.160 --> 00:51:57.270
and California is certainly
one of these places.

00:51:57.270 --> 00:52:00.540
Other countries might
lose everything,

00:52:00.540 --> 00:52:03.570
because their entire
economy depends

00:52:03.570 --> 00:52:08.170
on things like money or labor,
which will lose its importance,

00:52:08.170 --> 00:52:10.410
and they just don't
have the resources

00:52:10.410 --> 00:52:14.010
and the educational system
to kind of turn themselves

00:52:14.010 --> 00:52:15.870
into high-tech hubs.

00:52:15.870 --> 00:52:20.120
So the really crucial
question is not,

00:52:20.120 --> 00:52:23.650
what do we do
about, I don't know,

00:52:23.650 --> 00:52:26.940
Americans in Indiana
who lose their jobs?

00:52:26.940 --> 00:52:28.980
The really important
question is,

00:52:28.980 --> 00:52:32.790
what do we do about people
in Guatemala or Bangladesh

00:52:32.790 --> 00:52:34.710
who lose their jobs?

00:52:34.710 --> 00:52:36.870
This should be, I
think, the focus

00:52:36.870 --> 00:52:39.320
of this question of inequality.

00:52:39.320 --> 00:52:40.180
WILSON WHITE: OK.

00:52:40.180 --> 00:52:43.004
We'll take another
live question.

00:52:43.004 --> 00:52:44.170
AUDIENCE: Hello, Mr. Harari.

00:52:44.170 --> 00:52:46.859
Thank you for doing
this Q&amp;A. So at Google,

00:52:46.859 --> 00:52:49.150
we have a responsibility to
build products and services

00:52:49.150 --> 00:52:51.316
which not only achieve
results for our shareholders,

00:52:51.316 --> 00:52:53.450
but also that actually
benefit our end users.

00:52:53.450 --> 00:52:57.060
So in order to spend
less time hacking humans

00:52:57.060 --> 00:52:59.060
and spend more time
reducing suffering,

00:52:59.060 --> 00:53:01.790
we need to understand what type
of future we want to build.

00:53:01.790 --> 00:53:03.291
So what I wanted
to ask you is, what

00:53:03.291 --> 00:53:05.539
are your personal methodologies
for making predictions

00:53:05.539 --> 00:53:06.340
about the future?

00:53:06.340 --> 00:53:07.420
And what suggestions
would you give

00:53:07.420 --> 00:53:09.753
to Googlers who want to have
a more versed understanding

00:53:09.753 --> 00:53:10.925
of the future?

00:53:10.925 --> 00:53:13.050
YUVAL NOAH HARARI: As I
said in the very beginning,

00:53:13.050 --> 00:53:14.990
I don't think we can
predict the future,

00:53:14.990 --> 00:53:17.610
but I think we can influence it.

00:53:17.610 --> 00:53:21.720
What I try to do as a
historian-- and even

00:53:21.720 --> 00:53:25.530
when I talk about the future,
I define myself as a historian,

00:53:25.530 --> 00:53:29.400
because I think that history
is not the study of the past.

00:53:29.400 --> 00:53:32.070
History is the
study of change, how

00:53:32.070 --> 00:53:36.810
human societies and political
systems and economies change.

00:53:36.810 --> 00:53:41.670
And what I try to do is to
map different possibilities

00:53:41.670 --> 00:53:43.170
rather than make predictions.

00:53:43.170 --> 00:53:45.780
This is what will
happen in 2050.

00:53:45.780 --> 00:53:50.100
And we need to keep a
very broad perspective.

00:53:50.100 --> 00:53:52.190
One of the biggest
dangers is when

00:53:52.190 --> 00:53:54.390
we have a very
narrow perspective,

00:53:54.390 --> 00:53:57.660
like we develop a new
technology and we think,

00:53:57.660 --> 00:54:01.950
oh, this technology
will have this outcome.

00:54:01.950 --> 00:54:04.800
And we are convinced
of this prediction,

00:54:04.800 --> 00:54:08.790
and we don't take into account
that the same technology might

00:54:08.790 --> 00:54:12.120
have very different outcomes.

00:54:12.120 --> 00:54:15.280
And then we don't prepare.

00:54:15.280 --> 00:54:17.800
And again, as I said
in the beginning,

00:54:17.800 --> 00:54:20.340
it's especially important
to take into account

00:54:20.340 --> 00:54:26.080
the worst possible outcomes
in order to be aware of them.

00:54:26.080 --> 00:54:28.500
So I would say whenever
you are thinking

00:54:28.500 --> 00:54:32.250
about the future, the future
impact of a technology

00:54:32.250 --> 00:54:36.480
and developing, create a map
of different possibilities.

00:54:36.480 --> 00:54:40.530
If you see just one possibility,
you're not looking wide enough.

00:54:40.530 --> 00:54:43.950
If you see two or three, it's
probably also not wide enough.

00:54:43.950 --> 00:54:47.160
You need a map of, like, four
or five different possibilities,

00:54:47.160 --> 00:54:49.072
minimum.

00:54:49.072 --> 00:54:51.030
WILSON WHITE: Let's take
another live question.

00:54:53.940 --> 00:54:55.890
AUDIENCE: Hey, Mr. Harari.

00:54:55.890 --> 00:54:57.060
So my question is--

00:54:57.060 --> 00:54:59.860
I'll start very
broad, and then I'll

00:54:59.860 --> 00:55:01.680
narrow it down for the focus.

00:55:01.680 --> 00:55:03.156
I'm really interested
in, what do

00:55:03.156 --> 00:55:04.530
you think are the
components that

00:55:04.530 --> 00:55:08.910
make these fictional
stories so powerful in how

00:55:08.910 --> 00:55:11.480
they guide human nature?

00:55:11.480 --> 00:55:13.350
And then if I narrow
it down is, I'm

00:55:13.350 --> 00:55:16.170
specifically interested in
the self-destruction behavior

00:55:16.170 --> 00:55:17.280
of humans.

00:55:17.280 --> 00:55:23.170
How can these fictional
stories led by a few people

00:55:23.170 --> 00:55:27.250
convince the mass to
literally kill or die

00:55:27.250 --> 00:55:29.570
for that fictional story?

00:55:29.570 --> 00:55:32.410
YUVAL NOAH HARARI: It again
goes back to hacking the brain

00:55:32.410 --> 00:55:36.040
and hacking the human animal.

00:55:36.040 --> 00:55:39.130
It's been done throughout
history, previously just

00:55:39.130 --> 00:55:45.370
by trial and error, without the
deep knowledge of brain science

00:55:45.370 --> 00:55:46.930
and evolution we have today.

00:55:46.930 --> 00:55:49.090
But to give an
example, like if you

00:55:49.090 --> 00:55:53.260
want to convince
people to persecute

00:55:53.260 --> 00:55:57.790
and exterminate some other group
of people, what you need to do

00:55:57.790 --> 00:56:04.270
is really latch onto the disgust
mechanisms in the human brain.

00:56:04.270 --> 00:56:08.980
Evolution has
shaped homo sapiens

00:56:08.980 --> 00:56:12.460
with very powerful disgust
mechanisms in the brain

00:56:12.460 --> 00:56:17.950
to protect us against diseases,
against all kinds of sources

00:56:17.950 --> 00:56:20.420
of potential disease.

00:56:20.420 --> 00:56:24.760
And if you look at the
history of bias and prejudice

00:56:24.760 --> 00:56:27.520
and genocide, one
recurring theme

00:56:27.520 --> 00:56:31.780
is that it repeatedly
kind of latches

00:56:31.780 --> 00:56:35.200
onto these disgust mechanisms.

00:56:35.200 --> 00:56:40.720
And so you would find things
like women are impure,

00:56:40.720 --> 00:56:44.260
or these other
people, they smell bad

00:56:44.260 --> 00:56:46.120
and they bring diseases.

00:56:46.120 --> 00:56:50.890
And very, very often
disgust is at the center.

00:56:50.890 --> 00:56:56.620
So you'll often find comparison
between certain types of humans

00:56:56.620 --> 00:56:59.920
and rats or cockroaches,
or all kinds

00:56:59.920 --> 00:57:02.030
of other disgusting things.

00:57:02.030 --> 00:57:05.530
So if you want to
instigate genocide,

00:57:05.530 --> 00:57:11.470
you start by hacking the disgust
mechanisms in the human brain.

00:57:11.470 --> 00:57:14.920
And this is very, very deep.

00:57:14.920 --> 00:57:17.320
And if it's done
from an early age,

00:57:17.320 --> 00:57:20.350
it's extremely
difficult afterwards.

00:57:20.350 --> 00:57:23.440
People can-- they
know intellectually

00:57:23.440 --> 00:57:27.460
that it's wrong to say that
these people are disgusting,

00:57:27.460 --> 00:57:31.480
that these people,
they smell bad.

00:57:31.480 --> 00:57:33.370
But they know it intellectually.

00:57:33.370 --> 00:57:37.720
But when you place them,
like, in a brain scanner,

00:57:37.720 --> 00:57:39.430
they can't help it.

00:57:39.430 --> 00:57:41.070
If they were raised--

00:57:41.070 --> 00:57:43.250
I mean, so we can still
do something about it.

00:57:43.250 --> 00:57:45.160
We can still kind
of defeat this.

00:57:45.160 --> 00:57:47.320
But it's very difficult,
because it really

00:57:47.320 --> 00:57:49.820
goes to the core of the brain.

00:57:49.820 --> 00:57:52.070
WILSON WHITE: So I'll
end on a final question,

00:57:52.070 --> 00:57:54.020
because we're at time.

00:57:54.020 --> 00:57:56.060
When Larry and Sergey,
when they founded Google,

00:57:56.060 --> 00:57:59.300
they did so with
this deep belief

00:57:59.300 --> 00:58:03.530
in technology's ability
to improve people's lives

00:58:03.530 --> 00:58:05.100
everywhere.

00:58:05.100 --> 00:58:10.070
So if you had a magic wand
and you could give Google

00:58:10.070 --> 00:58:14.135
the next big project for us to
work on, in 30 seconds or less,

00:58:14.135 --> 00:58:18.020
what would you grant
us as our assignment?

00:58:18.020 --> 00:58:20.090
YUVAL NOAH HARARI:
An AI system that

00:58:20.090 --> 00:58:24.590
gets to know me in order to
protect me and not in order

00:58:24.590 --> 00:58:28.820
to sell me products or make me
click on advertisements and so

00:58:28.820 --> 00:58:29.560
forth.

00:58:29.560 --> 00:58:30.560
WILSON WHITE: All right.

00:58:30.560 --> 00:58:31.268
Mission accepted.

00:58:31.268 --> 00:58:32.290
[LAUGH]

00:58:32.290 --> 00:58:33.190
Thank you, guys.

00:58:33.190 --> 00:58:38.040
[APPLAUSE]

