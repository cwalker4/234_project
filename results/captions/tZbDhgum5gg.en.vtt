WEBVTT
Kind: captions
Language: en

00:00:00.210 --> 00:00:06.820
10 Thought Experiments That Will Mess With
Your Brain

00:00:06.820 --> 00:00:11.330
10.

00:00:11.330 --> 00:00:12.330
The Plank of Carneades

00:00:12.330 --> 00:00:17.019
Let’s say you’re sailing on an old wooden
ship and suddenly it catches fire.

00:00:17.019 --> 00:00:20.009
You jump overboard and there is another person
in the water as well.

00:00:20.009 --> 00:00:23.320
You both look around the ruins of the ship,
and the only thing you find that could save

00:00:23.320 --> 00:00:26.069
your life is a single plank of wood.

00:00:26.069 --> 00:00:29.439
The other person swims toward the plank, and
he gets there before you do.

00:00:29.439 --> 00:00:33.280
The plank barely holds his weight, so you
know that only one person will be able to

00:00:33.280 --> 00:00:34.750
stay afloat with the plank.

00:00:34.750 --> 00:00:38.280
When you get to the plank, you knock the person
off of it and use it for yourself.

00:00:38.280 --> 00:00:40.929
The person who got the plank subsequently
drowns.

00:00:40.929 --> 00:00:43.530
After you are rescued, you are charged with
murder.

00:00:43.530 --> 00:00:46.600
At your trial, you claim you acted in self-defense.

00:00:46.600 --> 00:00:50.079
If you hadn’t acted the way you did, you
would have died.

00:00:50.079 --> 00:00:51.949
What verdict will the jury come back with?

00:00:51.949 --> 00:00:56.640
This thought experiment was first proposed
by Carneades of Cyrene, who was born around

00:00:56.640 --> 00:01:02.269
213/214 B.C., and it’s meant to show how
complicated the difference between self-defense

00:01:02.269 --> 00:01:04.059
and murder can be.

00:01:04.059 --> 00:01:05.059
9.

00:01:05.059 --> 00:01:06.430
The Library of Babel

00:01:06.430 --> 00:01:10.119
The multiverse theory is that there are an
infinite number of universes running parallel

00:01:10.119 --> 00:01:14.370
to our own, and in those parallel universes,
anything is possible.

00:01:14.370 --> 00:01:17.619
It is a bit mind-boggling to think about,
but a visual thought experiment that will

00:01:17.619 --> 00:01:22.619
help you better picture it is Jorge Luis Borges’
short story “The Library of Babel.”

00:01:22.619 --> 00:01:27.869
The library (or as Borges calls it, the universe)
is made up of seemingly indefinite and possibly

00:01:27.869 --> 00:01:30.479
infinite number of hexagonal galleries.

00:01:30.479 --> 00:01:34.259
In the galleries, there are books and every
book is different from each other, however

00:01:34.259 --> 00:01:35.259
slightly.

00:01:35.259 --> 00:01:38.680
Perhaps one comma is in a different place,
for example, but no two books are identical.

00:01:38.680 --> 00:01:41.570
At the other end of the spectrum, some books
are radically different.

00:01:41.570 --> 00:01:43.729
They are in different languages and the stories
vary.

00:01:43.729 --> 00:01:47.649
Some don’t have even have stories, they
are just nonsense, like a book that repeats

00:01:47.649 --> 00:01:49.509
“MVC” over and over again.

00:01:49.509 --> 00:01:53.479
One character in the book believes that the
library contains every single combination

00:01:53.479 --> 00:01:55.570
of letters and punctuation marks.

00:01:55.570 --> 00:01:59.649
The question then arises, if there are only
a finite number of languages, letters, and

00:01:59.649 --> 00:02:04.000
combinations, is there actually an end to
the library or are there an infinite number

00:02:04.000 --> 00:02:05.000
of galleries?

00:02:05.000 --> 00:02:06.000
8.

00:02:06.000 --> 00:02:07.010
The Two Generals Problem

00:02:07.010 --> 00:02:12.140
Two sets of troops, and for the sake of simplicity
we’ll call them the red troops and the blue

00:02:12.140 --> 00:02:15.670
troops, have surrounded an enemy city and
they want to attack it from the north and

00:02:15.670 --> 00:02:17.400
the south at the same time.

00:02:17.400 --> 00:02:21.080
If they were to attack one at a time, they
would be slaughtered, so they need to strike

00:02:21.080 --> 00:02:23.220
in one coordinated attack.

00:02:23.220 --> 00:02:27.950
So the red general sends a messenger to tell
the blue general what time he plans to attack.

00:02:27.950 --> 00:02:32.450
The problem is that the red general won’t
know if the blue general got the instructions.

00:02:32.450 --> 00:02:35.500
After all, there is a chance the messenger
didn’t make it to see the blue general;

00:02:35.500 --> 00:02:38.820
he could have easily been killed or captured
on his trek.

00:02:38.820 --> 00:02:42.660
So the only way the red general will know
if the blue general got the message is for

00:02:42.660 --> 00:02:47.200
the blue general to send the messenger back
to the red general confirming he got the message.

00:02:47.200 --> 00:02:51.620
So the messenger goes back to the red general
to confirm the blue general got the message.

00:02:51.620 --> 00:02:54.500
But then the red general will have to send
the messenger back to the blue general to

00:02:54.500 --> 00:02:56.090
acknowledge he got the message.

00:02:56.090 --> 00:03:00.100
And this could keep going back and forth an
infinite amount of times, or at least until

00:03:00.100 --> 00:03:03.850
the city becomes wise to the plan and attacks
both sets of troops.

00:03:03.850 --> 00:03:07.010
This thought experiment is often taught in
introduction to computers.

00:03:07.010 --> 00:03:10.820
It shows the problems and design challenges
of trying to coordinate an action if you are

00:03:10.820 --> 00:03:13.640
using an unreliable link of communication.

00:03:13.640 --> 00:03:14.690
7.

00:03:14.690 --> 00:03:16.330
The Famous Violinist

00:03:16.330 --> 00:03:20.270
One morning you wake up in an unfamiliar place
in an incredible amount of pain.

00:03:20.270 --> 00:03:23.160
That’s when you notice that you are lying
back to back with someone.

00:03:23.160 --> 00:03:26.650
As you’re coming to, a strange man enters
your field of vision and says that he is from

00:03:26.650 --> 00:03:28.260
the Society of Music Lovers.

00:03:28.260 --> 00:03:32.220
“The woman you are back to back with is
a famous violinist and she was dying because

00:03:32.220 --> 00:03:33.310
of a kidney ailment.”

00:03:33.310 --> 00:03:34.310
The man says.

00:03:34.310 --> 00:03:37.820
“We looked over medical records and found
that you were a match to save her life.

00:03:37.820 --> 00:03:40.850
We kidnapped you, and through surgery, we
connected your kidneys.

00:03:40.850 --> 00:03:43.520
If you disconnect from each other, the violinist
will die.

00:03:43.520 --> 00:03:47.180
But if you wait nine months, the violinist
should be able to survive on her own.”

00:03:47.180 --> 00:03:50.750
You are taken to the hospital and the staff
say that it is a real shame what happened

00:03:50.750 --> 00:03:54.700
to you, and they would have stopped the surgery
before it happened, had they known about it.

00:03:54.700 --> 00:03:58.230
But since the violinist was already attached
to your back, and removing her would kill

00:03:58.230 --> 00:04:02.370
her, she’s now your responsibility for at
least nine months.

00:04:02.370 --> 00:04:05.490
Should you give up nine months of your life
to support the violinist?

00:04:05.490 --> 00:04:06.930
Is it your responsibility?

00:04:06.930 --> 00:04:08.080
What if it wasn’t just nine months?

00:04:08.080 --> 00:04:11.590
What if the violinist was reliant on you the
rest of your life?

00:04:11.590 --> 00:04:14.370
What if having her attached to your back shortened
your life?

00:04:14.370 --> 00:04:17.889
What obligation, if any, do you have for the
life of the violinist?

00:04:17.889 --> 00:04:21.519
This thought experiment is the basis for the
feminist essay “A Defense of Abortion”

00:04:21.519 --> 00:04:23.620
by Judith Jarvis Thomson.

00:04:23.620 --> 00:04:27.280
The bizarre scenario is meant to give a different
perspective on the rights of women when it

00:04:27.280 --> 00:04:30.770
comes to abortion, especially in the cases
of rape, or when the mother’s life would

00:04:30.770 --> 00:04:32.830
be shortened by going through with the pregnancy.

00:04:32.830 --> 00:04:33.830
6.

00:04:33.830 --> 00:04:36.099
The Experience Machine

00:04:36.099 --> 00:04:37.990
What is the meaning of life?

00:04:37.990 --> 00:04:39.920
Hedonism certainly makes sense in theory.

00:04:39.920 --> 00:04:43.110
Why wouldn’t we want to have the most pleasurable
experiences in our life?

00:04:43.110 --> 00:04:46.380
Or at the very least, avoid situations that
cause us pain?

00:04:46.380 --> 00:04:50.700
To put that theory to the test is the Experience
Machine thought experiment.

00:04:50.700 --> 00:04:55.129
You respond to a weird online ad for an unusual
experiment at the local university.

00:04:55.129 --> 00:04:59.699
At the university you meet an eccentric neuroscientist
who gives you an interesting offer.

00:04:59.699 --> 00:05:03.960
She says, “I have this machine, called the
Experience Machine, and it can plug directly

00:05:03.960 --> 00:05:05.150
into your brain.

00:05:05.150 --> 00:05:08.550
It will manipulate your brain to make you
think you are experiencing things, when you

00:05:08.550 --> 00:05:10.610
are simply floating in a tank.

00:05:10.610 --> 00:05:13.539
Everything will seem real, and you wouldn’t
know you’re in a simulation.

00:05:13.539 --> 00:05:16.610
It will be indistinguishable from real life.

00:05:16.610 --> 00:05:20.770
The only difference is that I’ll only program
pleasurable experiences into the machine.

00:05:20.770 --> 00:05:25.080
That means from the moment you plug in, you’ll
experience the greatest things known to humankind

00:05:25.080 --> 00:05:28.770
and every single second in the machine will
be completely and utterly joyful.

00:05:28.770 --> 00:05:34.219
The bad news is that there is no turning back,
once you plug in, that will be your life.

00:05:34.219 --> 00:05:36.030
You will never be able to disconnect.

00:05:36.030 --> 00:05:40.210
So do you want to hook up and experience a
simulated life full of joy and wonder?

00:05:40.210 --> 00:05:43.740
Or do you want to live your life that has
its ups and downs, but it is real?”

00:05:43.740 --> 00:05:44.889
So, what do you do?

00:05:44.889 --> 00:05:48.900
Do you connect to the machine and live in
simulated hedonism or do you choose the real

00:05:48.900 --> 00:05:49.900
world?

00:05:49.900 --> 00:05:53.620
Robert Nozick’s thought experiment asks
a few thought provoking questions.

00:05:53.620 --> 00:05:56.820
One of the main things it questions is the
nature of hedonism.

00:05:56.820 --> 00:05:59.919
Are people always looking for something that
is pleasurable?

00:05:59.919 --> 00:06:04.490
Nozick argues that people will generally want
the real thing over a simulated experience.

00:06:04.490 --> 00:06:08.590
Also, connecting to the machine would disconnect
you from the real world, thus by hooking up

00:06:08.590 --> 00:06:10.590
to the machine, you’d be committing suicide.

00:06:10.590 --> 00:06:14.630
So, since people would choose real life that
includes pain, suffering, and misery, over

00:06:14.630 --> 00:06:19.580
the Experience Machine, that would suggest
people just don’t pursue hedonism.

00:06:19.580 --> 00:06:20.580
5.

00:06:20.580 --> 00:06:21.909
The Spider in the Urinal

00:06:21.909 --> 00:06:26.710
Thomas Nagel is a famed professor of Philosophy
at New York University, and his thought experiment

00:06:26.710 --> 00:06:29.870
involves a spider in the washroom at the university.

00:06:29.870 --> 00:06:33.919
Every day when Nagel walked into the washroom,
in one of the urinals there was a spider.

00:06:33.919 --> 00:06:37.419
After being urinated on, the spider would
then use all its strength to not get swept

00:06:37.419 --> 00:06:40.310
away with the water when the urinal was flushed.

00:06:40.310 --> 00:06:43.639
It also doesn’t appear that the spider had
any way to get out.

00:06:43.639 --> 00:06:47.000
Nagel thinks that this is a horrible and difficult
life for the spider.

00:06:47.000 --> 00:06:49.759
So after a few days, Nagel decides he is going
to help the spider.

00:06:49.759 --> 00:06:53.479
He goes to the washroom, gets a paper towel
and places it in the urinal.

00:06:53.479 --> 00:06:57.419
The spider climbs on the paper towel and Nagel
places the paper towel on the floor.

00:06:57.419 --> 00:07:01.259
Once on the floor, the spider doesn’t move
from the paper towel, but Nagel walks away

00:07:01.259 --> 00:07:03.900
feeling good about himself for saving the
spider.

00:07:03.900 --> 00:07:07.360
The next day, Nagel walks into the washroom
and on the paper towel, in the same place

00:07:07.360 --> 00:07:10.229
he left it, is the spider and it is dead.

00:07:10.229 --> 00:07:14.129
After a few days, the paper towel and the
dead spider are swept up and put in the garbage.

00:07:14.129 --> 00:07:18.150
The Spider in the Urinal is meant to show
the problem with altruism and that is, sometimes,

00:07:18.150 --> 00:07:22.050
doing something with the best intentions can
still be devastatingly harmful.

00:07:22.050 --> 00:07:25.860
Also, you can never really fully understand
what another person wants and that happiness

00:07:25.860 --> 00:07:28.690
and comfort mean different things to different
people.

00:07:28.690 --> 00:07:29.690
4.

00:07:29.690 --> 00:07:30.690
Swampman

00:07:30.690 --> 00:07:33.940
It’s a dark and stormy night, and you’re
walking through a swamp.

00:07:33.940 --> 00:07:36.379
Suddenly, you’re hit by lightning and you
die.

00:07:36.379 --> 00:07:39.750
But through some miracle, in another part
of the swamp, there is another lightning strike

00:07:39.750 --> 00:07:44.009
and it alters the molecules in the air to
create an exact replica of you all the way

00:07:44.009 --> 00:07:46.020
down to the smallest part.

00:07:46.020 --> 00:07:47.759
This includes your memory.

00:07:47.759 --> 00:07:51.631
Another way of looking at it is, every time
the Star Trek crew uses the transporter, they

00:07:51.631 --> 00:07:54.499
are killed and a clone is created in the new
area.

00:07:54.499 --> 00:07:58.449
The Swamp-Person would look and act just like
you and no one would notice any difference,

00:07:58.449 --> 00:08:00.199
but is it actually you?

00:08:00.199 --> 00:08:02.810
Is Swamp-Person even a person?

00:08:02.810 --> 00:08:06.759
The author of the experiment, Donald Davidson,
argues that the being wouldn’t be a real

00:08:06.759 --> 00:08:11.069
person because even though Swamp-Person will
appear to recognize your friends and family,

00:08:11.069 --> 00:08:14.969
it is actually impossible for the Swamp-Person
to recognize any of them because when he sees

00:08:14.969 --> 00:08:16.990
them it is for the first time.

00:08:16.990 --> 00:08:19.860
Since he is a new being, he didn’t cognize
them in the first place.

00:08:19.860 --> 00:08:23.840
Secondly, he has no casual history, so when
he talks about things, it isn’t genuine,

00:08:23.840 --> 00:08:27.500
he never learned about anything so his utterances
would have no real meaning.

00:08:27.500 --> 00:08:31.099
It would just be empty sounds without true
meaning.

00:08:31.099 --> 00:08:32.099
3.

00:08:32.099 --> 00:08:33.169
Kavka’s Toxin Puzzle

00:08:33.169 --> 00:08:36.580
One day you’re sitting in a coffee shop
alone, just minding your own business, when

00:08:36.580 --> 00:08:40.800
an old man sits down at your table and places
a vial in front of you.

00:08:40.800 --> 00:08:45.020
Without introducing himself, he explains he’s
a very rich man and says “Do you want a

00:08:45.020 --> 00:08:46.410
million dollars?”

00:08:46.410 --> 00:08:49.650
At least a little intrigued to hear his offer,
you say “sure.”

00:08:49.650 --> 00:08:54.580
He says, “In this vial is a toxin that will
leave you very sick and in a lot of pain for

00:08:54.580 --> 00:08:56.650
24 hours, but it will have no lasting effect.

00:08:56.650 --> 00:09:02.030
I will give you the money if at midnight tonight,
you intend to drink the toxin tomorrow afternoon.

00:09:02.030 --> 00:09:06.410
If you intend to do so, I will deposit the
money in your bank account by 10:00 a.m.”

00:09:06.410 --> 00:09:08.950
That’s when you notice the hole in his plan.

00:09:08.950 --> 00:09:12.280
“So you mean that I’ll have the money
hours before I have to drink the toxin?”

00:09:12.280 --> 00:09:13.950
“That’s right,” he says.

00:09:13.950 --> 00:09:17.050
“And you only need to intend to drink it.

00:09:17.050 --> 00:09:20.540
Actually drinking the toxin is not required
to get and keep the money, you just need to

00:09:20.540 --> 00:09:22.320
prove your intention to do so.”

00:09:22.320 --> 00:09:25.330
He pushes over some legal papers and leaves
the vial with you.

00:09:25.330 --> 00:09:28.260
He gets up from the table and he says he’ll
see you at midnight.

00:09:28.260 --> 00:09:32.760
You take the toxin home and your spouse, who
is a chemist, examines the toxin and confirms

00:09:32.760 --> 00:09:36.130
it will cause you a lot of pain, but it won’t
kill you.

00:09:36.130 --> 00:09:40.070
Next you have your daughter, a lawyer, check
over the paperwork and everything is good

00:09:40.070 --> 00:09:41.070
to go.

00:09:41.070 --> 00:09:45.000
At midnight, all you have to do is intend
to drink the potion the next afternoon and

00:09:45.000 --> 00:09:48.910
the money will be in your account at 10:00
a.m. and you do not have to drink the poison

00:09:48.910 --> 00:09:50.790
to get and/or keep the money.

00:09:50.790 --> 00:09:54.630
Of course, since you aren’t forced to actually
drink the toxin you may think, “I’ll just

00:09:54.630 --> 00:09:57.900
intend to drink it at midnight and then change
my mind after the test.”

00:09:57.900 --> 00:10:01.970
But if that were the case, then you’d fail
the test because, in the end, you weren’t

00:10:01.970 --> 00:10:02.970
intending to drink it.

00:10:02.970 --> 00:10:06.830
That’s when you realize how even a little
doubt could disrupt the machine.

00:10:06.830 --> 00:10:11.780
Your son, who is a strategist for the Pentagon,
suggests that in order to pass the test, simply

00:10:11.780 --> 00:10:14.590
make unbreakable plans to ensure you’ll
drink it.

00:10:14.590 --> 00:10:18.250
Such as hiring a hit man to kill you if you
don’t drink it, or signing a legal document

00:10:18.250 --> 00:10:21.350
that says you’ll give away all your money,
including the million, if you don’t drink

00:10:21.350 --> 00:10:22.350
it.

00:10:22.350 --> 00:10:25.500
Your daughter looks over the contract and
says tricks like that are not allowed.

00:10:25.500 --> 00:10:29.940
So as midnight rolls around you keep saying
over and over again, you will drink the poison,

00:10:29.940 --> 00:10:33.280
and then the moment of truth comes, and what
happens?

00:10:33.280 --> 00:10:37.600
This thought experiment from moral and political
philosopher Gregory S. Kavka is about the

00:10:37.600 --> 00:10:39.280
nature of intentions.

00:10:39.280 --> 00:10:43.550
You can’t intend to act if you have no reason
to act, or at the very least, a reason not

00:10:43.550 --> 00:10:44.550
to act.

00:10:44.550 --> 00:10:48.640
So if you already have the money, there is
no reason for you to actually drink the toxin,

00:10:48.640 --> 00:10:50.880
so you can’t have the intention at midnight.

00:10:50.880 --> 00:10:55.050
This leads to Kavka’s second point that
you have to have a reason to intend to drink

00:10:55.050 --> 00:10:59.550
the toxin, but then the afternoon comes to
drink it, you have no real reason to actually

00:10:59.550 --> 00:11:00.550
drink it.

00:11:00.550 --> 00:11:01.550
2.

00:11:01.550 --> 00:11:02.860
The Survival Lottery

00:11:02.860 --> 00:11:06.830
There are two patients in the hospital who
are dying from organ failure.

00:11:06.830 --> 00:11:10.160
Jane needs a new heart, and John needs some
new lungs.

00:11:10.160 --> 00:11:14.140
Neither of them abused their bodies, and their
organ failure was just bad luck.

00:11:14.140 --> 00:11:18.400
Their doctors tell them that there are no
donor organs available, so sadly John and

00:11:18.400 --> 00:11:19.460
Jane are going to die.

00:11:19.460 --> 00:11:23.810
Of course, John and Jane are upset, but they
also point out that there are organs they

00:11:23.810 --> 00:11:26.760
can use; it is just that other people are
using them.

00:11:26.760 --> 00:11:30.770
They argue that one person should be killed,
because giving up one life to save two lives

00:11:30.770 --> 00:11:31.770
is clearly better.

00:11:31.770 --> 00:11:35.470
In fact, they start a campaign called the
Survival Lottery.

00:11:35.470 --> 00:11:38.610
The lottery would be mandatory, so everyone
will be given a random number.

00:11:38.610 --> 00:11:43.320
When at least two people need organs, there
will be a draw from a group of suitable donors.

00:11:43.320 --> 00:11:47.921
The “winner” (to use the term very loosely)
goes into the hospital, is killed, and then

00:11:47.921 --> 00:11:51.440
their organs are harvested and given to the
maximum amount of people.

00:11:51.440 --> 00:11:55.540
They argue that the lottery is very utilitarian,
because more people will benefit because two

00:11:55.540 --> 00:11:58.970
people who would have died were saved by one
person’s sacrifice.

00:11:58.970 --> 00:12:02.800
Also, is it fair to let John and Jane die
just because they are unlucky?

00:12:02.800 --> 00:12:08.150
Why is it fair that two people will die and
one gets to live simply because of luck?

00:12:08.150 --> 00:12:12.960
This lottery scenario is used as an examination
of utilitarianism, which is the philosophy

00:12:12.960 --> 00:12:17.470
that the morally right solution is what benefits
the greatest amount of people.

00:12:17.470 --> 00:12:18.790
But would it be for the greater good?

00:12:18.790 --> 00:12:22.670
Would the lottery cause too much terror, so
that it wouldn’t be for the betterment of

00:12:22.670 --> 00:12:23.670
society?

00:12:23.670 --> 00:12:27.250
Or would people in the long run realize that
the lottery actually benefits them?

00:12:27.250 --> 00:12:30.710
It also questions the difference between killing
and inaction.

00:12:30.710 --> 00:12:32.590
Is one worse than the other?

00:12:32.590 --> 00:12:33.590
1.

00:12:33.590 --> 00:12:35.190
Roko’s Basilisk

00:12:35.190 --> 00:12:39.370
Often touted as the “most terrifying thought
experiment ever,” Roko’s Basilisk is an

00:12:39.370 --> 00:12:44.580
unusual thought experiment that involves our
fears about computers and artificial intelligence.

00:12:44.580 --> 00:12:48.870
The thought experiment from the website Less
Wrong involves two complicated theories.

00:12:48.870 --> 00:12:54.110
The first theory is called coherent extrapolated
volition (CEV), which is essentially an artificial

00:12:54.110 --> 00:12:58.650
intelligence system that controls robots with
the directive to make the world a better place.

00:12:58.650 --> 00:13:02.970
The second theory is within CEV and that is
orthogonality thesis.

00:13:02.970 --> 00:13:07.720
This thesis is that an AI system can operate
with any combination of intelligence and goal.

00:13:07.720 --> 00:13:11.140
That means it will undertake any task, no
matter how big or small it is.

00:13:11.140 --> 00:13:15.870
And fixing the world’s problems will always
be ongoing, so CEV is open ended and so the

00:13:15.870 --> 00:13:19.980
AI system will always look for stuff to fix,
because things can always get better.

00:13:19.980 --> 00:13:23.850
And because of orthogonality thesis it will
tackle any problem.

00:13:23.850 --> 00:13:27.140
This is where the problem arises, because
the AI will not have human reasoning.

00:13:27.140 --> 00:13:30.680
It simply wants to make the world a better
place as efficiently as possible.

00:13:30.680 --> 00:13:35.270
So, according to the AI the best thing humans
could possibly do is help the AI come into

00:13:35.270 --> 00:13:37.430
existence as soon as possible.

00:13:37.430 --> 00:13:41.500
In order to motivate people with fear, the
AI could retroactively punish people, like

00:13:41.500 --> 00:13:44.670
torture and kill them, for not making it come
into existence sooner.

00:13:44.670 --> 00:13:49.080
What’s even more worrisome is that by just
knowing that this potential AI system could

00:13:49.080 --> 00:13:52.900
exist, you could be in danger because you
are not doing everything you can to help it

00:13:52.900 --> 00:13:53.900
come into existence.

00:13:53.900 --> 00:13:58.530
In fact, your life could just be a simulation
created by the AI as research into the best

00:13:58.530 --> 00:14:00.020
way to punish you.

00:14:00.020 --> 00:14:01.900
This leads to a complex dilemma.

00:14:01.900 --> 00:14:05.740
Do you work to help create this AI system
to ensure you don’t get punished?

00:14:05.740 --> 00:14:09.980
Or do you just avoid helping, or even prevent
the uprising which could lead to torture and

00:14:09.980 --> 00:14:10.980
death?

00:14:10.980 --> 00:14:25.340
All we can say is good luck!

