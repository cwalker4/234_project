{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Preprocessing\n",
    "\n",
    "This notebook does some pre-processing of the unstructured data gathered by the scraper. The goal is to be able to move things to R and avoid matplotlib :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the raw video info json\n",
    "raw = pd.read_json('results/video_info.json', orient='index')\n",
    "raw.reset_index(inplace=True)\n",
    "raw.rename(index=str, columns={\"index\": \"video_id\"}, inplace=True)\n",
    "raw.drop(['comments', 'description'], axis=1).to_csv('derived_data/video_info.csv',\n",
    "                                                    index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis of Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process the comments\n",
    "comments = raw[['video_id', 'comments']].copy()\n",
    "\n",
    "comments = comments.comments.apply(lambda x: pd.Series(x))\\\n",
    "                        .stack()\\\n",
    "                        .reset_index(level=1, drop=True)\\\n",
    "                        .to_frame('comments')\\\n",
    "                        .join(comments[['video_id']], how='left')\n",
    "\n",
    "comments['comments'] = comments['comments'].astype('str')\n",
    "\n",
    "polarities = comments['polarity'] = comments.comments\\\n",
    "                       .apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "    \n",
    "avg_polarity = comments.groupby('video_id')['polarity'].mean().reset_index()\n",
    "avg_polarity.to_csv('derived_data/comment_sentiments.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis of Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions = raw[['video_id', 'description']].copy()\n",
    "descriptions['description'] = descriptions.description.astype(str)\n",
    "\n",
    "descriptions['polarity'] = descriptions.description.\\\n",
    "                           apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "    \n",
    "descriptions[['video_id', 'polarity']].to_csv('derived_data/description_sentiments.csv',\n",
    "                                             index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis of Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "polarities = {}\n",
    "\n",
    "for file in os.listdir('derived_data/captions_clean/'):\n",
    "    if file[0] == '.':\n",
    "        continue\n",
    "    video_id = file.split('.')[0]\n",
    "    captions = open(os.path.join('derived_data/captions_clean', file), 'r').readlines()\n",
    "    captions = [line.replace('\\n', '') for line in captions]\n",
    "    blob = TextBlob(\" \".join(captions))\n",
    "    polarities[video_id] = blob.sentiment.polarity\n",
    "    \n",
    "polarity_df = pd.DataFrame(list(polarities.items()), columns=['video_id', 'polarity'])\n",
    "polarity_df.to_csv('derived_data/caption_sentiments.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Misc Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a category id <-> category crosswalk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/category_info.json', 'r') as f:\n",
    "    raw = json.load(f)\n",
    "\n",
    "result = {}\n",
    "for item in raw.get('items', []):\n",
    "    result[item['id']] = item['snippet']['title']\n",
    "    \n",
    "\n",
    "crosswalk = pd.DataFrame.from_dict(result, orient=\"index\").reset_index()\n",
    "crosswalk.rename(index=str, columns={'index': 'category_id',\n",
    "                                     0: 'category_name'},\n",
    "                inplace=True)\n",
    "\n",
    "crosswalk.to_csv('derived_data/category_crosswalk.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make an adjacency list \n",
    "\n",
    "Combine the BFS tree searches into one big ol' adjacency list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary of out-edges\n",
    "result = {}\n",
    "\n",
    "# populate the dictionary\n",
    "for folder in os.listdir('results'):\n",
    "    if not os.path.isdir(os.path.join('results', folder)) or folder == \"captions\":\n",
    "        continue\n",
    "        \n",
    "    with open(os.path.join('results', folder, 'search_info.json'), 'r') as f:\n",
    "        search_info = json.load(f)\n",
    "        \n",
    "    for video_id in search_info:\n",
    "        if video_id in result:\n",
    "            result[video_id].union(set(search_info[video_id]['recommendations']))\n",
    "        else:\n",
    "            result[video_id] = set(search_info[video_id]['recommendations'])\n",
    "\n",
    "# save as adjacency list\n",
    "f = open('derived_data/adjacency_list.txt', 'w')\n",
    "for video_id in result:\n",
    "    line = \"{} {}\".format(video_id, \" \".join(result[video_id]))\n",
    "    f.write(line + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make search info dataframe\n",
    "\n",
    "The json search_info format was nice for scraping, not nice for analyzing. Pack it into one csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['video_id', 'recommendations', 'depth', 'search', 'query',\n",
    "           'search_splits', 'search_depth', 'root_video']\n",
    "result = pd.DataFrame(columns=columns)\n",
    "\n",
    "for folder in os.listdir('results'):\n",
    "    if not os.path.isdir(os.path.join('results', folder)) or folder == \"captions\":\n",
    "        continue\n",
    "    \n",
    "    with open(os.path.join('results', folder, 'params.json'), 'r') as f:\n",
    "        params = json.load(f)\n",
    "    \n",
    "    filepath = os.path.join('results', folder, 'search_info.json')\n",
    "    search_df = pd.read_json(filepath, orient='index').reset_index()\n",
    "    search_df.rename(index=str, columns={'index': 'video_id'}, inplace=True)\n",
    "    \n",
    "    search_df['search'] = folder\n",
    "    search_df['query'] = folder.split(\"_\")[0]\n",
    "    search_df['search_splits'] = params['n_splits']\n",
    "    search_df['search_depth'] = params['depth']\n",
    "    search_df['root_video'] = folder.split(\"_\")[1]\n",
    "    \n",
    "    result = result.append(search_df, ignore_index=True)\n",
    "    \n",
    "result.to_csv('derived_data/search_info.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open adjacency list\n",
    "f = open('derived_data/adjacency_list.txt', 'r')\n",
    "\n",
    "in_degrees = {}\n",
    "out_degrees = {}\n",
    "\n",
    "for line in f.read().splitlines():\n",
    "    out_degrees[line.split(\" \")[0]] = len(line.split(\" \")[1:])\n",
    "    for ix, video_id in enumerate(line.split(\" \")):\n",
    "        if ix == 0 or video_id == \"\":\n",
    "            continue\n",
    "        if video_id in in_degrees:\n",
    "            in_degrees[video_id] += 1\n",
    "        else:\n",
    "            in_degrees[video_id] = 1\n",
    "            \n",
    "in_deg = pd.DataFrame.from_dict(in_degrees, orient=\"index\")\n",
    "in_deg = in_deg.rename(index=str, columns={0: 'in_deg'})\n",
    "\n",
    "out_deg = pd.DataFrame.from_dict(out_degrees, orient=\"index\")\n",
    "out_deg = out_deg.rename(index=str, columns={0: 'out_deg'})\n",
    "\n",
    "full = in_deg.join(out_deg, how='left').reset_index()\\\n",
    "             .rename(index=str, columns={'index': 'video_id'})\n",
    "    \n",
    "full.to_csv('derived_data/vertex_degrees.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
